{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "Experiment parameters\n",
      "=====================\n",
      "        exp_dir = .\n",
      " param_filename = params.json\n",
      "output_filename = penalty10_decay0.3+s047_16000-16020.json\n",
      "\n",
      "\n",
      "Start time is:  2020-03-18 10:00:45.004406\n",
      "\n",
      "Number of jumps: 440\n",
      "\n",
      "per node: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "INFERENCE\n",
      "=========\n",
      "\n",
      "Run VI (vi_exp)\n",
      "------\n",
      "vi random seed: 49763\n",
      "end_iter is: 38777\n",
      "Converged!\n",
      "\n",
      "\n",
      "\n",
      "Save results...\n",
      "\n",
      "coeffs: [-0.8448057272405971, -3.7908382147137276, -3.937382848440482, -4.166887589098733, -3.590380059766666, -4.088614330265292, -4.302683145132499, -4.561431080400771, -4.617051842586872, -4.544160531734658, -4.67943512866706, -3.7798805415170884, -4.2043047901642705, -4.154528299664775, -4.142895160715641, -4.036282034750953, -3.9101032791853263, -3.776099461860967, -3.7064554411578685, -3.6273646408050166, -3.3754206467199053, -2.9375606176746927, -0.0033477302406905184, -3.8624057566874903, -4.15531509400623, -4.145130962904007, -4.012741303858782, -3.9102341531364413, -3.7629182322155743, -3.7337600306466663, -3.6308411101814464, -3.375056266553646, -2.932101809368322, -2.593911375890361, -0.054429557755876296, -3.822571845961221, -4.164367718293616, -4.039392261627372, -3.8939091184967567, -3.774022327530371, -3.711157182110254, -3.6329575776624496, -3.380785727822689, -2.9367518214899397, -3.067654669942735, -2.468592123974638, -0.06899738613218034, -3.858218120946949, -4.041025815161449, -3.912537088721317, -3.771745609305962, -3.7049644556969636, -3.6319920315095464, -3.3871494538842, -2.959200075227668, -3.1659913145360843, -2.680834367989487, -1.7935158191095482, -0.2605726275469603, -3.572269483162446, -3.912293351765621, -3.750426193810018, -3.7102183300345497, -3.6435200140102895, -3.3803911886386473, -2.9485085154491353, -3.648865221228833, -3.450679976088325, -3.198478704599342, -3.1059782352927523, 0.07835713424584452, -3.5054546214927798, -3.764490337710477, -3.6884099213759423, -3.650535399145535, -3.3836937599158854, -2.9657831945994126, -3.8243466079108948, -3.7153418279320833, -3.5633908704214363, -3.4952426919816304, -2.8169872696080103, 0.19767143670062987, -3.363168420703469, -3.7199176608895934, -3.645271099993747, -3.3732762780058163, -2.953518363095253, -3.9818751161863952, -3.8759185936356575, -3.766110141108349, -3.721664964507741, -3.3114371711285018, -2.703916287350829, 0.3445565186456622, -3.412651667348151, -3.643403325753636, -3.3645265415386665, -2.9591513497836273, -4.006112281134451, -3.908784473217745, -3.82096072238575, -3.7782729129004324, -3.4291709444157634, -2.902473979664255, -1.8061217666191172, 0.28758989443715166, -3.3091431786963734, -3.3912073174387634, -2.930425470258072, -4.038852919612212, -3.97513107642145, -3.8959549153652024, -3.8409941988817637, -3.5204136054306687, -3.1285603723116147, -2.3769834898209283, -1.9249437922547818, 0.32983168788827927, -2.8963396455338906, -2.954132476031806, -4.090764512887247, -4.0582844763899795, -3.988715563132987, -3.9506565216723617, -3.7448061583510395, -3.478049022994965, -3.0963151973648073, -2.936281757820816, -2.631150253451657, 0.6981852910227554, -2.271520443723441, -1.964612234850067, -0.11653782516527791, -0.09695904577598329, -0.03961749627092809, -0.1867201112972535, -0.07254840014365015, -0.03794756490010971, -0.021206193202381405, -0.018718353229827754, -0.0008983186044038735, -0.02294926827828129, 0.002512701459116219, -0.007500783225129513, 0.012868145794373932, -0.004627902320374381, 0.015938945879885164, 0.0028063759408268967, 0.0028557405016348078, -0.0005846124258235542, 0.007894084176839832, 0.012515084192769215, -0.0014054272715047422, -1.8950980009794989, -0.02242260482335977, 0.011893693021879351, 0.008814959108028621, -0.0015604211870251193, 0.014854463256308952, 0.01439890850150157, -0.0024713266399590283, 0.000753505709073881, 0.005807621713968888, 0.000175297880880258, -0.1709742474721074, -1.7965017496642586, -0.01062559892990554, 0.008443823354025095, -0.014191708269934804, 0.015539645554411292, -0.007388601558354604, -0.0015577539727984707, 0.013917641886364575, 0.0002356070805959693, 0.017299909577325197, -0.09216301416582849, -0.22250885165084347, -1.7340746767350879, -0.010138286143203017, -0.007368273456294165, 0.0016978090225109722, 0.014741883149436173, 0.012464264628829666, 0.013807476183211395, 0.010897512474105087, -0.009222327611392878, -0.07892563857382179, -0.16691652727376857, -0.44769517349884214, -1.5602136686801766, -0.0008192410719484157, 0.00763332700819044, 0.014810683636627537, -0.0065242977397175226, -0.006253513997402831, 0.0075917881159547045, 0.00015698717445155632, -0.012720959094070212, -0.025461795139467492, -0.043202510645062135, -0.054528046005793286, -1.7864420767672056, -0.015213456745299657, -0.01000647831556544, 0.024067354586261507, 0.0024903736386188893, 0.0067247929969081275, 0.007122093142582359, 0.018711680631798008, -0.015803423707489453, -0.004453566311745795, -0.0023492717319059746, -0.08992702149773502, -1.754768452064124, 0.006348584563968955, -0.0025844658851492228, 0.010186079231655406, 0.00712014421511655, -0.004174910966074657, 0.005213524911951589, 0.018370555339140077, -0.004028383989370922, -0.010727563785796862, -0.019810377129973248, -0.07900038238265604, -1.7783800831729664, -0.02406018348212776, 0.01579530053402354, -0.007154363808953543, -0.008986708293697661, 0.018013044793863866, 0.003472879687234088, -0.011078094682559197, 0.012002230641421058, -0.019850557219666, -0.04637570212484197, -0.27815952850181547, -1.6696343406614933, 0.02345690805589467, 0.012243713459176306, 0.014308886555579142, -0.017843426529679445, 0.021020130555252696, -0.0019280281202960757, 0.000829305802909758, -0.011442167599141866, -0.022726361776934597, -0.12109970819904228, -0.2274764626070521, -1.6198878089371602, -0.014993967530847174, 0.009078020354486014, 0.011943936304469773, -0.023810663919204268, 0.0019250940021661085, 0.0016594695864303392, -0.009401838161163556, -0.009743546346791466, -0.029036461423768197, -0.030207709169016373, -0.03559162202253163, -1.7516477015590748, -0.046889950617302306]\n",
      "\n",
      "adjacency: [0.008354803860181323, 0.005575266351966613, 0.005624638334311592, 0.0058948154057415665, 0.006290759496750981, 0.007330351489356888, 0.008380671097433878, 0.00904731817370484, 0.00962609055515185, 0.012267930476427082, 0.019550513832877217, 0.9743946196537345, 0.008078482534991658, 0.005631453742186087, 0.005724974375548229, 0.006673403186895128, 0.007151817257062552, 0.00829473157615023, 0.008836807835061408, 0.009731869556541898, 0.01244123115831394, 0.019595556128385774, 0.03672505100579169, 0.921322518595061, 0.008217043026942929, 0.005620139645603399, 0.0066615181489682405, 0.007259267044824287, 0.008571146527730854, 0.009022385407073346, 0.00945526185318476, 0.012509623473188826, 0.018836530590626323, 0.020255695143069862, 0.04462749833274692, 0.90468174805455, 0.00792172941945481, 0.00656241029995208, 0.007328836708988953, 0.008216028601353103, 0.008824650337090095, 0.009466539924475353, 0.01216510434441478, 0.019430243536335524, 0.017953537775275814, 0.03347328228403796, 0.1105857600350191, 0.7373364854531999, 0.010351412008875793, 0.007243308973552586, 0.008391880583356909, 0.009120282510077021, 0.009744123692852477, 0.012330361777548128, 0.019277386786736742, 0.009815973990428377, 0.012264685300678433, 0.016314493966549243, 0.018267014082015618, 1.0515679850557258, 0.011384688422925723, 0.008697876957047252, 0.008758576220665514, 0.009508885792471151, 0.012311343590924163, 0.01868322320330045, 0.007731293921645494, 0.00923985343249936, 0.010519513807181205, 0.011214416180288369, 0.02592935891400229, 1.182653536940402, 0.012576232685282794, 0.008961998185719315, 0.009411224679609708, 0.012430297467253992, 0.019347282587859366, 0.006789644351013383, 0.0073478832413969245, 0.00858202188855292, 0.009091290961308814, 0.013945602481880393, 0.02850244470325218, 1.3716651057019311, 0.012706129863966789, 0.009320880262189265, 0.012902686967569237, 0.019422200885291115, 0.006455678332085046, 0.0073301858380833695, 0.008237584027099006, 0.008208641809462052, 0.012397659271997154, 0.0220621010548809, 0.09260267102563334, 1.2867594601913814, 0.012814539420872411, 0.012082501577306708, 0.01907347474100604, 0.006712422703277977, 0.006617302005745349, 0.007505596894389459, 0.007886095080356835, 0.01113358424870539, 0.016837863487213268, 0.04234592307611617, 0.07734946431866933, 1.3373084895131517, 0.020925288245703316, 0.01882716811737262, 0.006006338286698435, 0.00665904460752476, 0.006788174854190228, 0.0070552661129534905, 0.0088602688707065, 0.011576818606515323, 0.01759929677557827, 0.020699305160542875, 0.028369476685835514, 1.9505029382360577, 0.04150241834264416]\n",
      "\n",
      "mu: [0.42127702715058657, 0.010224874368530543, 0.008556122075733344, 0.006153646804491475, 0.013860084433781027, 0.007058286989103801, 0.00535568560647867, 0.004006226441960234, 0.0037714066657655618, 0.003917249586602463, 0.003572198674263383]\n",
      "\n",
      "\n",
      "Finished.\n",
      "\n",
      "End time is:  2020-03-18 12:22:16.401198\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.optimize import fsolve\n",
    "from scipy import log\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "# External libraries\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Internal libraries\n",
    "\n",
    "import models\n",
    "import posteriors\n",
    "import priors\n",
    "import hawkes_model, excitation_kernels\n",
    "import learners\n",
    "import utils\n",
    "\n",
    "from make_data_for_samples import make_data              #多个样本数据\n",
    "from make_data_for_estimate import make_estimate_data    #单个样本数据\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "#def learn_vi(events, end_time, vi_seed, adjacency_true, inference_param_dict, return_learner=False):\n",
    "def learn_vi(events, vi_seed, inference_param_dict, return_learner=False):\n",
    "    # Extract some parameters for easier access\n",
    "    #n_nodes = len(events)\n",
    "    n_events = len(events)\n",
    "    n_nodes = len(events[0])\n",
    "    M = inference_param_dict['excitation']['args'].get('M', 1)\n",
    "    n_params = n_nodes * (n_nodes * M + 1)\n",
    "    n_edges = M * n_nodes ** 2\n",
    "    # Set seed\n",
    "    np.random.seed(vi_seed)\n",
    "    # Set starting pointM * n_nodes ** 2\n",
    "    x0 = torch.tensor(\n",
    "        np.hstack((\n",
    "            np.hstack((  # alpha, the mean of the parameters\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_nodes),\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_edges),)),\n",
    "            np.hstack((  # beta=log(sigma), log of the variance of the parameters\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_nodes), 1e-1, 2.0)),\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_edges), 1e-1, 2.0)),))\n",
    "        )),\n",
    "        dtype=torch.float64, requires_grad=True\n",
    "    )\n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **inference_param_dict['excitation'])\n",
    "    hawkes_model_obj = hawkes_model.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "    # Init the posterior object\n",
    "    posterior_obj = make_object(posteriors, **inference_param_dict['posterior'])\n",
    "    # Init the prior object\n",
    "    prior_type = inference_param_dict['prior']['name']\n",
    "    prior_args = inference_param_dict['prior']['args']\n",
    "    prior_args['C'] = torch.tensor(prior_args['C'], dtype=torch.float64)  # cast to tensor\n",
    "    prior_obj = make_object(priors, prior_type, prior_args)\n",
    "    # Init the variational inference model object\n",
    "    model = models.ModelHawkesVariational(\n",
    "        model=hawkes_model_obj, posterior=posterior_obj, prior=prior_obj,\n",
    "        **inference_param_dict['model']['args'])\n",
    "   \n",
    "    # Init the optimizer\n",
    "    opt_type = inference_param_dict['optimizer']['name']\n",
    "    opt_args = inference_param_dict['optimizer']['args']\n",
    "    opt = getattr(optim, opt_type)([x0], **opt_args)\n",
    "    # Init learner\n",
    "    learner = learners.VariationalInferenceLearner(\n",
    "        model=model, optimizer=opt, **inference_param_dict['learner']['args'])\n",
    "    # Fit the model\n",
    "    events_t = [torch.tensor(events_i) for events_i in events]  # cast to tensor\n",
    "    #learner.fit(events_t, end_time, x0, callback=callback)\n",
    "    learner.fit(events_t, x0=x0, callback=None)\n",
    "    print()\n",
    "    if return_learner:\n",
    "        return learner\n",
    "    # Extract the mode of the posterior\n",
    "    z_est_mode = learner.model.posterior.mode(learner.coeffs[:n_params], learner.coeffs[n_params:])\n",
    "    adj_est_ora = z_est_mode[n_nodes:].detach()\n",
    "    mu_est_ora = z_est_mode[:n_nodes].detach()\n",
    "    adj_est_ora = adj_est_ora.view(n_nodes, n_nodes, M)\n",
    "    adj_est = z_est_mode[n_nodes:].detach().numpy()\n",
    "    adj_est = np.reshape(adj_est, (n_nodes, n_nodes, M)).sum(-1).ravel()\n",
    "    mu_est = z_est_mode[:n_nodes].detach().numpy()\n",
    "    #mu_est = np.reshape(mu_est,n_nodes).ravel()\n",
    "    coeffs_est = learner.coeffs.detach().numpy()\n",
    "    #log_like_sum,min_intens,log_like,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    log_like_sum,intens_sum,integral_instesity,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    \n",
    "    return coeffs_est, adj_est,mu_est,intens_sum,integral_instesity,end_time\n",
    "\n",
    "\n",
    "def run(exp_dir, param_filename, output_filename, stdout=None, stderr=None):\n",
    "    # Reset random seed\n",
    "    np.random.seed(None)\n",
    "\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "\n",
    "    print('\\nExperiment parameters')\n",
    "    print('=====================')\n",
    "    print(f'        exp_dir = {exp_dir:s}')\n",
    "    print(f' param_filename = {param_filename:s}')\n",
    "    print(f'output_filename = {output_filename:s}')\n",
    "    print(flush=True)\n",
    "    print('\\nStart time is: ', datetime.datetime.today())\n",
    "\n",
    "    result_dict = {}\n",
    "    \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    global events\n",
    "    events = make_data('s047',16000,16020,data_fileName)\n",
    "    n_jumps_per_dim = list(map(len, events[0]))\n",
    "    print('\\nNumber of jumps:', len(events)*sum(n_jumps_per_dim))\n",
    "    print('\\nper node:', n_jumps_per_dim)\n",
    "    \n",
    "    C_list = [1.0]*132\n",
    "    \n",
    "    param_dict={'inference':{'vi_exp':{'excitation': {'name': 'ExponentialKernel','args': {'decay': 0.3, 'cut_off': 1000.0}}, \n",
    "                          'posterior': {'name': 'LogNormalPosterior', 'args': {}},\n",
    "                          'prior': {'name': 'GaussianLaplacianPrior', 'args': {'dim': 11, 'n_params': 132, 'C': C_list}}, \n",
    "                          'model': {'args': {'n_samples': 1, 'n_weights': 1, 'weight_temp': 1.0}}, \n",
    "                          'optimizer': {'name': 'Adam', 'args': {'lr': 0.01}}, \n",
    "                          'learner': {'args': {'tol': 1e-04, 'lr_gamma': 0.9999, 'max_iter': 40000, 'hyperparam_momentum': 0.5, 'hyperparam_interval': 100, 'hyperparam_offset': 0}}}}\n",
    "               }\n",
    "    \n",
    "\n",
    "    print('\\nINFERENCE')\n",
    "    print('=========')\n",
    "\n",
    "    for key, inference_param_dict in param_dict['inference'].items():\n",
    "        if key.startswith('vi'):\n",
    "            print(f'\\nRun VI ({key:s})')\n",
    "            print('------')\n",
    "            # Set random seed (for reproducibility)\n",
    "            np.random.seed()  # Reset random number generator to avoid dependency on simulation seed\n",
    "            #vi_seed = np.random.randint(2**32 - 1)\n",
    "            vi_seed = np.random.randint(2**16 - 1)\n",
    "            print(f'vi random seed: {vi_seed}')\n",
    "            # Run inference\n",
    "            global intens_sum\n",
    "            global integral_instesity\n",
    "            #coeffs_var, adj_var, mu_var, nu, varsigma = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            coeffs_var, adj_var, mu_var,intens_sum,integral_instesity,end_time  = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            #模型参数\n",
    "            adj_var = adj_var.ravel()\n",
    "            mu_var = mu_var.ravel()            \n",
    "          \n",
    "            global  end_time_result\n",
    "            end_time_result = [0.0]*len(end_time)\n",
    "            \n",
    "            for i in range(len(end_time)):\n",
    "                end_time_result[i]=end_time[i].tolist()\n",
    "                \n",
    "            expresion_temp = ''\n",
    "            events_n = len(events)\n",
    "            dim = len(events[0])\n",
    "            \n",
    "            for i in range(events_n):\n",
    "                temp = ''                  \n",
    "                for j in range(dim):\n",
    "                    temp += 'log('+str(intens_sum[i][j].detach().numpy())+ '-'+ 'epsilon_noise['+str(i)+'])'+ '+'   \n",
    "                #print(intens)\n",
    "                expresion_temp += temp[:-1] + '-' + str(integral_instesity[i].detach().numpy()) + '+' + str(dim) +'*'+ str(end_time_result[i])+'*'+ 'epsilon_noise['+str(i)+'],'\n",
    "\n",
    "            expresion = expresion_temp[:-1]\n",
    "            expresion = '['+expresion+']'\n",
    "         \n",
    "            result_dict.update({\n",
    "                key: {\n",
    "                    'vi_seed': vi_seed,             # VI random seed\n",
    "                    'coeffs': coeffs_var.tolist(),  # VI parameters\n",
    "                    'adjacency': adj_var.tolist(),  # VI Estimator\n",
    "                    'mu':  mu_var.tolist(),\n",
    "                    'expresion': expresion,\n",
    "                }\n",
    "            })\n",
    " \n",
    "\n",
    "    print('\\n\\nSave results...')\n",
    "    \n",
    "    print('\\ncoeffs:',  coeffs_var.tolist())\n",
    "    print( '\\nadjacency:', adj_var.tolist())\n",
    "    print('\\nmu:', mu_var.tolist())\n",
    "    #print('\\nnu:',nu)\n",
    "    #print('\\nvarsigma:',varsigma)\n",
    "\n",
    "    with open(os.path.join(exp_dir, output_filename), 'w') as output_file:\n",
    "        json.dump(result_dict, output_file)\n",
    "\n",
    "    # Log that the run is finished\n",
    "    print('\\n\\nFinished.')\n",
    "    print('\\nEnd time is: ', datetime.datetime.today())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dir', dest='dir', type=str,\n",
    "                        #required=True, help=\"Working directory\")\n",
    "                        required=False, default=\".\")\n",
    "    parser.add_argument('-p', '--params', dest='param_filename', type=str,\n",
    "                        required=False, default='params.json',\n",
    "                        help=\"Input parameter file (JSON)\")\n",
    "    parser.add_argument('-o', '--outfile', dest='output_filename', type=str,\n",
    "                        required=False, default='output.json',\n",
    "                        help=\"Output file (JSON)\")\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    #run(exp_dir=args.dir, param_filename=args.param_filename,output_filename=args.output_filename)\n",
    "    run ('.','params.json','penalty10_decay0.3+s047_16000-16020.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
