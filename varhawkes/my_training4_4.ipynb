{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "Experiment parameters\n",
      "=====================\n",
      "        exp_dir = .\n",
      " param_filename = params.json\n",
      "output_filename = penalty10_decay0.5+s002_0-20.json\n",
      "\n",
      "\n",
      "Start time is:  2020-03-18 10:03:45.334414\n",
      "\n",
      "Number of jumps: 440\n",
      "\n",
      "per node: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "INFERENCE\n",
      "=========\n",
      "\n",
      "Run VI (vi_exp)\n",
      "------\n",
      "vi random seed: 63842\n",
      "end_iter is: 37429\n",
      "Converged!\n",
      "\n",
      "\n",
      "\n",
      "Save results...\n",
      "\n",
      "coeffs: [-0.5725750826985324, -3.842835226605351, -3.89543041987674, -3.9188224964346987, -3.028765785494192, -3.287255407325484, -4.129527208299456, -4.343901023569472, -4.353456671634878, -4.439403122970006, -4.517819061541668, -3.805411106869594, -4.276101753477845, -4.250085122134977, -4.235445258180302, -4.158276749894211, -3.937908125732418, -3.7738503616166357, -3.675609051014647, -3.5911128023094956, -3.3617847616821774, -2.9593773325628803, -0.08225017944712164, -3.9902167995464355, -4.274685431472153, -4.255884669615832, -4.144921387556566, -3.968126371134498, -3.7813427642299793, -3.6708946929516615, -3.590277469394913, -3.380133205130558, -2.991427486630629, -2.325995349391073, -0.18157746303385663, -3.9638943203221944, -4.263536138599258, -4.1647295236315545, -3.9346059741310477, -3.766731790867572, -3.6568858962746233, -3.5759116185700344, -3.3766596329978893, -2.956230948240578, -2.745831806532153, -2.1892369154715907, -0.2906255467673615, -3.955918866650706, -4.143471721726102, -3.9343721534070344, -3.775399848795195, -3.695068385848348, -3.6025501553726333, -3.3845709560048816, -2.9708677468042892, -3.014279329943036, -2.6440108771773154, -2.165258130541095, -0.4371660661239824, -3.648333120908589, -3.9477141759343475, -3.7903408505085325, -3.6777963169337817, -3.596122189961434, -3.387537213298288, -2.9587328484210524, -3.729736487590663, -3.6244468620278867, -3.5347197884344603, -3.436620026492056, -0.07921787076196303, -3.32224146922234, -3.77311958575045, -3.6594159404711943, -3.5887300542944085, -3.3779000986703855, -2.938468646594976, -4.074733901612526, -3.9685839174934134, -3.9496255695900175, -3.898108681382101, -3.4301091217694886, 0.21255855671512333, -3.348694736044152, -3.668004988493532, -3.5809324079765594, -3.383243138551162, -2.9730228013584803, -4.132311574011032, -4.112902236011454, -4.077049704792202, -4.040263507627131, -3.7253945985358086, -2.8196208650482637, 0.34638871796275017, -3.319435338251172, -3.4653220936528393, -3.3993247202945067, -2.9609947380312613, -4.128253635328156, -4.076479310200349, -4.058943502569823, -4.043314815845926, -3.7300916661313903, -2.8903690005606437, -1.081384814021954, 0.17756401000544075, -3.2290723337534226, -3.395846308705008, -2.982307806250383, -4.209686046804218, -4.154229316064347, -4.121706124886719, -4.121136289277601, -3.872140252593513, -3.3150764166434152, -2.6075859714914307, -1.8365461919321515, 0.4020862566993385, -2.9665124775082545, -2.958873327021469, -4.215746867051291, -4.198268765524735, -4.176286694124875, -4.155405720686316, -3.978670172287454, -3.5291299246131937, -3.0987268918952013, -2.758383592947709, -2.4579116403054067, 0.6772023664247617, -2.4066998185427395, -1.9323325025900573, -0.06673768481917748, -0.060219389471696604, -0.05273017791336603, -0.26135635914091376, -0.1731253202906773, -0.00822792146465268, -0.0034045959264124145, -0.015387437153663865, -0.000496929817756897, -0.009580442227966059, -0.02039862761478126, 0.00728823212176941, -0.0066074849532507425, 0.015132477920068168, 0.016072545597494558, 0.009066134157590248, -0.0006747346326341382, 0.004215734101339674, 0.008801720399317346, 0.01592323238375706, 0.025772080816872083, -1.877344885326297, -0.002175350017107209, -0.00818257282984843, 0.015678501810579983, 0.01390449105229689, 0.006130085231548503, -0.0015493683534942167, 0.005260308216618896, 0.023728302090288096, 0.008875090844513453, -0.02106310918044422, -0.2823572201332673, -1.756747910501115, 0.004761431920403797, -0.014266963674903477, -0.002857683279546169, 0.0023660617809405727, 0.027557717763557133, 0.004319880145165101, -0.004109019970346978, -0.00683951929998957, 0.017512898291218288, -0.16172561164871557, -0.33771826925005904, -1.6501452923425077, -0.014506463926301108, -0.007317548209314833, 0.0031176913789599733, 0.008221660498922083, 0.0018005996525950684, -0.0002445601323193458, -0.0026926803112785045, 0.016275290447487222, -0.08930061747433103, -0.19110123698744202, -0.3435681861015329, -1.5166632632630042, -0.019718175091152586, 0.02572190190672807, -0.0019247331375867194, -0.007546302072193137, -5.975760471724527e-05, 0.007564326644305944, -0.0016953006921251123, 0.0011256300117586221, -0.018576468479708865, -0.01928289188356025, -0.049045421831202994, -1.718509424000768, -0.01663523025863784, -0.0035440989810661115, 0.024025770216574587, -0.002195037773578268, 0.01680452237530817, 0.01925119497205268, -0.004309516470612438, 0.005687677489623478, -0.0077539320674469905, 0.001754719672291458, -0.009977993189570697, -1.8170726484154052, -0.004715054908803095, -0.002692750766158279, 0.00772600008782522, 0.0023860015613800674, -0.004497007329164648, 0.013486847288043639, 0.0032206553623671543, 0.022741669636360054, 0.0030315441022888702, 0.0056739119527910425, -0.06843064555707154, -1.7767222052994887, 0.007216297207787352, -0.006339658510735884, -0.010903845398496986, 0.005144688970720182, 0.018158965676731652, 0.018686289912407998, -0.014519965478003726, -0.004105253327299943, 0.013150850001948487, -0.052679018978050454, -0.6616665035970541, -1.5342228873959511, 0.01310736823122955, 0.003483460261700761, -0.0037233870944227017, 0.006488565828412201, 0.017673849926640593, 0.010775402330087972, 0.008859978144744655, 0.007810131290171796, -0.03207397019710934, -0.08064767425366681, -0.2519864439394854, -1.6619883292410718, -0.021937465274021038, 0.0047814347839711755, -0.009038415403054969, 0.008001283651706213, -0.007140312638165339, 0.018452545860321197, 0.0020118968575527736, -0.0004203342907952167, -0.025020481220761507, -0.04297140985613174, -0.07984005843653108, -1.7282614777434366, -0.030602565951146306]\n",
      "\n",
      "adjacency: [0.008519182811514685, 0.005037804169712795, 0.005316409621842529, 0.005163333642967665, 0.005566748903423634, 0.007039586954773151, 0.00845919044117295, 0.009241268084573958, 0.009963049327629927, 0.012349449999648533, 0.018092224558297097, 0.8997324022623345, 0.006833792080024716, 0.005203345643474268, 0.005053175187362877, 0.005666856927256421, 0.006870887435807116, 0.008410714723479758, 0.009265375921910748, 0.009668538053659064, 0.012301404236709619, 0.01925130847427646, 0.055325695602770834, 0.8094744689233278, 0.006919147774544239, 0.00532466511143512, 0.005747269200083548, 0.007159255299478124, 0.008039451858607615, 0.009413945860149922, 0.01038152749655457, 0.012739170672506205, 0.01846501584008945, 0.03113324496755862, 0.06732853614180738, 0.7207247341346961, 0.007245876193290489, 0.005922816412881284, 0.007150113726258006, 0.008296022422495972, 0.009107326520343897, 0.010031138693306152, 0.012535382034153737, 0.01824336378961516, 0.02126444806642488, 0.03592335072584646, 0.06937195600396612, 0.6154994370211929, 0.009955160077310975, 0.006734550597470295, 0.008341612099985367, 0.00943983256904167, 0.010092096543658955, 0.01224324547388862, 0.019152002710617875, 0.008808921329389974, 0.01017343236394724, 0.011143619399614075, 0.012995052502617609, 0.8946002679930473, 0.013711527409127127, 0.008513888993169681, 0.009017026352808365, 0.01021038845462638, 0.012129918096755803, 0.018728256389693974, 0.006306642057949819, 0.006873896084507183, 0.007195946963533007, 0.00743449750801494, 0.012150913362915922, 1.2046054214688933, 0.013045540206169244, 0.009441560901329039, 0.010087034349542645, 0.012425225654780648, 0.018985701953620832, 0.005743696124313441, 0.005979822841700545, 0.005954647491848307, 0.006432802336235087, 0.008766592809826054, 0.024929285313487322, 1.3740503631934207, 0.013115342046829286, 0.011646811580410378, 0.012553533515220396, 0.018848211812465054, 0.005711696896941526, 0.006008626895196914, 0.0065367107842119075, 0.0065053008220671855, 0.008593559012279177, 0.022587215231443577, 0.2598550679755977, 1.1400483230199692, 0.014184099451378577, 0.012242551659911838, 0.018781391924064163, 0.005392491549120309, 0.005570841427366936, 0.005837277190713615, 0.005863467214993095, 0.007537361068274407, 0.01422232085068513, 0.03147295567606522, 0.08710230307873365, 1.4420661432725201, 0.01977006946451745, 0.01890210349784256, 0.005528534862263202, 0.005437694790630137, 0.0057296114901742145, 0.005555314396075704, 0.006855513415115974, 0.010799131018982795, 0.0174238146144572, 0.02532330163271782, 0.03650419624833329, 1.9072513685016685, 0.03517815149512223]\n",
      "\n",
      "mu: [0.5523656576218989, 0.008934060822528919, 0.008379544010858294, 0.008077017105115688, 0.026737832946897855, 0.018414984232583743, 0.006016762090344341, 0.004809726959103586, 0.0048773632481673825, 0.004346389593329385, 0.004091512471003401]\n",
      "\n",
      "\n",
      "Finished.\n",
      "\n",
      "End time is:  2020-03-18 12:20:37.000030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.optimize import fsolve\n",
    "from scipy import log\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "# External libraries\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Internal libraries\n",
    "\n",
    "import models\n",
    "import posteriors\n",
    "import priors\n",
    "import hawkes_model, excitation_kernels\n",
    "import learners\n",
    "import utils\n",
    "\n",
    "from make_data_for_samples import make_data              #多个样本数据\n",
    "from make_data_for_estimate import make_estimate_data    #单个样本数据\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "#def learn_vi(events, end_time, vi_seed, adjacency_true, inference_param_dict, return_learner=False):\n",
    "def learn_vi(events, vi_seed, inference_param_dict, return_learner=False):\n",
    "    # Extract some parameters for easier access\n",
    "    #n_nodes = len(events)\n",
    "    n_events = len(events)\n",
    "    n_nodes = len(events[0])\n",
    "    M = inference_param_dict['excitation']['args'].get('M', 1)\n",
    "    n_params = n_nodes * (n_nodes * M + 1)\n",
    "    n_edges = M * n_nodes ** 2\n",
    "    # Set seed\n",
    "    np.random.seed(vi_seed)\n",
    "    # Set starting pointM * n_nodes ** 2\n",
    "    x0 = torch.tensor(\n",
    "        np.hstack((\n",
    "            np.hstack((  # alpha, the mean of the parameters\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_nodes),\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_edges),)),\n",
    "            np.hstack((  # beta=log(sigma), log of the variance of the parameters\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_nodes), 1e-1, 2.0)),\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_edges), 1e-1, 2.0)),))\n",
    "        )),\n",
    "        dtype=torch.float64, requires_grad=True\n",
    "    )\n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **inference_param_dict['excitation'])\n",
    "    hawkes_model_obj = hawkes_model.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "    # Init the posterior object\n",
    "    posterior_obj = make_object(posteriors, **inference_param_dict['posterior'])\n",
    "    # Init the prior object\n",
    "    prior_type = inference_param_dict['prior']['name']\n",
    "    prior_args = inference_param_dict['prior']['args']\n",
    "    prior_args['C'] = torch.tensor(prior_args['C'], dtype=torch.float64)  # cast to tensor\n",
    "    prior_obj = make_object(priors, prior_type, prior_args)\n",
    "    # Init the variational inference model object\n",
    "    model = models.ModelHawkesVariational(\n",
    "        model=hawkes_model_obj, posterior=posterior_obj, prior=prior_obj,\n",
    "        **inference_param_dict['model']['args'])\n",
    "   \n",
    "    # Init the optimizer\n",
    "    opt_type = inference_param_dict['optimizer']['name']\n",
    "    opt_args = inference_param_dict['optimizer']['args']\n",
    "    opt = getattr(optim, opt_type)([x0], **opt_args)\n",
    "    # Init learner\n",
    "    learner = learners.VariationalInferenceLearner(\n",
    "        model=model, optimizer=opt, **inference_param_dict['learner']['args'])\n",
    "    # Fit the model\n",
    "    events_t = [torch.tensor(events_i) for events_i in events]  # cast to tensor\n",
    "    #learner.fit(events_t, end_time, x0, callback=callback)\n",
    "    learner.fit(events_t, x0=x0, callback=None)\n",
    "    print()\n",
    "    if return_learner:\n",
    "        return learner\n",
    "    # Extract the mode of the posterior\n",
    "    z_est_mode = learner.model.posterior.mode(learner.coeffs[:n_params], learner.coeffs[n_params:])\n",
    "    adj_est_ora = z_est_mode[n_nodes:].detach()\n",
    "    mu_est_ora = z_est_mode[:n_nodes].detach()\n",
    "    adj_est_ora = adj_est_ora.view(n_nodes, n_nodes, M)\n",
    "    adj_est = z_est_mode[n_nodes:].detach().numpy()\n",
    "    adj_est = np.reshape(adj_est, (n_nodes, n_nodes, M)).sum(-1).ravel()\n",
    "    mu_est = z_est_mode[:n_nodes].detach().numpy()\n",
    "    #mu_est = np.reshape(mu_est,n_nodes).ravel()\n",
    "    coeffs_est = learner.coeffs.detach().numpy()\n",
    "    #log_like_sum,min_intens,log_like,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    log_like_sum,intens_sum,integral_instesity,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    \n",
    "    return coeffs_est, adj_est,mu_est,intens_sum,integral_instesity,end_time\n",
    "\n",
    "\n",
    "def run(exp_dir, param_filename, output_filename, stdout=None, stderr=None):\n",
    "    # Reset random seed\n",
    "    np.random.seed(None)\n",
    "\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "\n",
    "    print('\\nExperiment parameters')\n",
    "    print('=====================')\n",
    "    print(f'        exp_dir = {exp_dir:s}')\n",
    "    print(f' param_filename = {param_filename:s}')\n",
    "    print(f'output_filename = {output_filename:s}')\n",
    "    print(flush=True)\n",
    "    print('\\nStart time is: ', datetime.datetime.today())\n",
    "\n",
    "    result_dict = {}\n",
    "    \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    global events\n",
    "    events = make_data('s002',0,20,data_fileName)\n",
    "    n_jumps_per_dim = list(map(len, events[0]))\n",
    "    print('\\nNumber of jumps:', len(events)*sum(n_jumps_per_dim))\n",
    "    print('\\nper node:', n_jumps_per_dim)\n",
    "    \n",
    "    C_list = [1.0]*132\n",
    "    \n",
    "    param_dict={'inference':{'vi_exp':{'excitation': {'name': 'ExponentialKernel','args': {'decay': 0.5, 'cut_off': 1000.0}}, \n",
    "                          'posterior': {'name': 'LogNormalPosterior', 'args': {}},\n",
    "                          'prior': {'name': 'GaussianLaplacianPrior', 'args': {'dim': 11, 'n_params': 132, 'C': C_list}}, \n",
    "                          'model': {'args': {'n_samples': 1, 'n_weights': 1, 'weight_temp': 1.0}}, \n",
    "                          'optimizer': {'name': 'Adam', 'args': {'lr': 0.01}}, \n",
    "                          'learner': {'args': {'tol': 1e-04, 'lr_gamma': 0.9999, 'max_iter': 40000, 'hyperparam_momentum': 0.5, 'hyperparam_interval': 100, 'hyperparam_offset': 0}}}}\n",
    "               }\n",
    "    \n",
    "\n",
    "    print('\\nINFERENCE')\n",
    "    print('=========')\n",
    "\n",
    "    for key, inference_param_dict in param_dict['inference'].items():\n",
    "        if key.startswith('vi'):\n",
    "            print(f'\\nRun VI ({key:s})')\n",
    "            print('------')\n",
    "            # Set random seed (for reproducibility)\n",
    "            np.random.seed()  # Reset random number generator to avoid dependency on simulation seed\n",
    "            #vi_seed = np.random.randint(2**32 - 1)\n",
    "            vi_seed = np.random.randint(2**16 - 1)\n",
    "            print(f'vi random seed: {vi_seed}')\n",
    "            # Run inference\n",
    "            global intens_sum\n",
    "            global integral_instesity\n",
    "            #coeffs_var, adj_var, mu_var, nu, varsigma = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            coeffs_var, adj_var, mu_var,intens_sum,integral_instesity,end_time  = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            #模型参数\n",
    "            adj_var = adj_var.ravel()\n",
    "            mu_var = mu_var.ravel()            \n",
    "          \n",
    "            global  end_time_result\n",
    "            end_time_result = [0.0]*len(end_time)\n",
    "            \n",
    "            for i in range(len(end_time)):\n",
    "                end_time_result[i]=end_time[i].tolist()\n",
    "                \n",
    "            expresion_temp = ''\n",
    "            events_n = len(events)\n",
    "            dim = len(events[0])\n",
    "            \n",
    "            for i in range(events_n):\n",
    "                temp = ''                  \n",
    "                for j in range(dim):\n",
    "                    temp += 'log('+str(intens_sum[i][j].detach().numpy())+ '-'+ 'epsilon_noise['+str(i)+'])'+ '+'   \n",
    "                #print(intens)\n",
    "                expresion_temp += temp[:-1] + '-' + str(integral_instesity[i].detach().numpy()) + '+' + str(dim) +'*'+ str(end_time_result[i])+'*'+ 'epsilon_noise['+str(i)+'],'\n",
    "\n",
    "            expresion = expresion_temp[:-1]\n",
    "            expresion = '['+expresion+']'\n",
    "         \n",
    "            result_dict.update({\n",
    "                key: {\n",
    "                    'vi_seed': vi_seed,             # VI random seed\n",
    "                    'coeffs': coeffs_var.tolist(),  # VI parameters\n",
    "                    'adjacency': adj_var.tolist(),  # VI Estimator\n",
    "                    'mu':  mu_var.tolist(),\n",
    "                    'expresion': expresion,\n",
    "                }\n",
    "            })\n",
    " \n",
    "\n",
    "    print('\\n\\nSave results...')\n",
    "    \n",
    "    print('\\ncoeffs:',  coeffs_var.tolist())\n",
    "    print( '\\nadjacency:', adj_var.tolist())\n",
    "    print('\\nmu:', mu_var.tolist())\n",
    "    #print('\\nnu:',nu)\n",
    "    #print('\\nvarsigma:',varsigma)\n",
    "\n",
    "    with open(os.path.join(exp_dir, output_filename), 'w') as output_file:\n",
    "        json.dump(result_dict, output_file)\n",
    "\n",
    "    # Log that the run is finished\n",
    "    print('\\n\\nFinished.')\n",
    "    print('\\nEnd time is: ', datetime.datetime.today())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dir', dest='dir', type=str,\n",
    "                        #required=True, help=\"Working directory\")\n",
    "                        required=False, default=\".\")\n",
    "    parser.add_argument('-p', '--params', dest='param_filename', type=str,\n",
    "                        required=False, default='params.json',\n",
    "                        help=\"Input parameter file (JSON)\")\n",
    "    parser.add_argument('-o', '--outfile', dest='output_filename', type=str,\n",
    "                        required=False, default='output.json',\n",
    "                        help=\"Output file (JSON)\")\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    #run(exp_dir=args.dir, param_filename=args.param_filename,output_filename=args.output_filename)\n",
    "    run ('.','params.json','penalty10_decay0.5+s002_0-20.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
