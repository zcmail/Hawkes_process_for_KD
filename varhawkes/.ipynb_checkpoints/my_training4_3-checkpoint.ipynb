{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "Experiment parameters\n",
      "=====================\n",
      "        exp_dir = .\n",
      " param_filename = params.json\n",
      "output_filename = penalty10_decay0.1+s036_12000-12050.json\n",
      "\n",
      "\n",
      "Start time is:  2020-03-14 13:36:50.117830\n",
      "\n",
      "Number of jumps: 1100\n",
      "\n",
      "per node: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "INFERENCE\n",
      "=========\n",
      "\n",
      "Run VI (vi_exp)\n",
      "------\n",
      "vi random seed: 64151\n",
      "end_iter is: 37742\n",
      "Converged!\n",
      "\n",
      "\n",
      "\n",
      "Save results...\n",
      "\n",
      "coeffs: [-1.1890361092356792, -4.298859276623802, -4.914582516679356, -5.056038696015544, -5.213235635634439, -5.389638925681673, -5.619926757456059, -5.8222451808488715, -5.868130753337182, -5.887788448811579, -5.999957008973599, -4.325242796604499, -4.591129183845102, -4.506799595798878, -4.456856282172697, -4.384377292861091, -4.232851857797042, -4.137240560630805, -4.059503277933725, -3.9927185551115407, -3.738481898210704, -2.913438222803671, 0.5528055195333568, -4.236842302100623, -4.530298754330236, -4.45215002148758, -4.366474029557211, -4.237760098391449, -4.12037543556789, -4.065068902444545, -3.972250163086841, -3.729792121407408, -2.9257897622771223, -2.910032667389201, 0.5912106705441348, -4.2191236122145765, -4.468422371364627, -4.359541456940443, -4.261279406335373, -4.147217639289107, -4.0442517491358645, -3.9880324326391836, -3.742187874413429, -2.9287145414905473, -3.3072868960153876, -2.4964824054154984, 0.6192961043276436, -4.119427554497328, -4.3767929429119, -4.241122742610427, -4.105816140984524, -4.052781562510242, -3.9763130965149167, -3.73580632099056, -2.912825816634434, -3.6483951989214205, -3.1559117396473546, -2.644827451450998, 0.676660162821621, -4.04320888300897, -4.248288675304536, -4.129999409246759, -4.050767421161866, -3.9722977871264393, -3.707222147142941, -2.93429206499924, -3.881891451333975, -3.5458405798553088, -3.2615655218780026, -2.670866926199455, 0.7544196895583638, -3.893449074760232, -4.132056104517023, -4.07062550584929, -3.9563432786642805, -3.754417685377097, -2.924908233944166, -4.070847548554508, -3.850996146221115, -3.609555688328358, -3.31742975963242, -2.771052691420657, 0.8705632256008976, -3.775670102284464, -4.0657133078641134, -3.9648097274819283, -3.733671596562715, -2.9353975166510597, -4.212577785538827, -4.012925087024184, -3.844863721911943, -3.602466907167138, -3.2833695286546107, -2.6203652923936596, 0.983650076537792, -3.744693548825337, -3.985819724388106, -3.7364329795522444, -2.920706006189834, -4.269376546066617, -4.10197070235802, -3.9383691711644246, -3.7305567341605737, -3.457383405972967, -2.9224502053174497, -1.9196502944748277, 0.9986726868503681, -3.650448935649021, -3.750962949532743, -2.942099307578656, -4.305864274701455, -4.148926132273264, -4.024354176915475, -3.867168575153986, -3.6254429812700635, -3.1967136596788874, -2.5276490658720054, -2.0429845299757936, 1.0760663979374843, -3.319530770238466, -2.90750200093946, -4.410410706854907, -4.306203950233764, -4.196409998249951, -4.063285243423411, -3.8687765438816766, -3.613398168078986, -3.2384693900232357, -3.058610933570175, -2.6958977877293244, 1.3402169821489338, -1.8844280862220208, -2.486211274001833, -0.23016271976295197, -0.09299029183536102, -0.0680211675427796, -0.06134734512074246, -0.024963066046910513, -0.020579055506167083, -0.006938340822484954, -0.0139868637628934, -0.004984001185006658, 0.00817488239152938, -0.0189729185186763, -0.000261830769928456, 0.00803931133554075, -0.0008032961184158578, 0.008096458560940734, -0.003868727887045625, -0.0059926470113953545, 0.015583951011427288, -0.0028405984692676496, 0.0067403124536555194, -0.001953489999425137, -2.4173300607387955, -0.01748543415687173, -0.0007224113808259844, -0.003327251308444174, -0.018039415816189324, 0.024079317160771267, 0.007313353343489109, 0.006630924176401986, 0.004373178441504465, 0.0011469566409388957, 0.02152989921304423, -0.14122027014111949, -2.3873518877498547, -0.013562835973902548, 0.0028685400680908456, 0.007638463838305298, 0.022214462676877156, 0.0071221071046046235, -0.003981026918294789, -0.01149588035168769, 0.004011846850285942, -0.011172055646877009, -0.05169736424080921, -0.19323400143216438, -2.3458386241327007, -0.0038650482553512734, -0.006371633223551935, 0.013239814025965805, 0.020366032604364488, 0.006955353878091646, 0.006613024738684413, 0.01354885324659647, -0.0005112787418448609, -0.030706278447079505, -0.07823659535573915, -0.14894394216174467, -2.3172362457064244, -0.005423296068871718, 0.0007527040841893357, -0.021410470258034813, -0.0026045774677740494, 0.002405586087534944, 0.024274764123509145, 0.0006051109551302934, -0.01539652914594513, -0.01324697488773749, -0.06427098833850362, -0.1125495903561954, -2.3194891584498594, 0.011870440741323839, -0.002394273900248993, 0.004976919034356693, 0.006383715978328854, 0.0045038327850688105, 0.01561012169212078, 0.019896251020217644, -0.01320582270891201, -0.00950687898233609, -0.055610916348396246, -0.12160641752452457, -2.302078580403197, -0.0013282085892673086, -0.012617032338635604, 0.0036045328294163756, 0.01380954673725373, 0.006205960646415589, 0.004811297475847428, -0.0035022969908415145, 0.001740126510407076, -0.0012688753046761088, -0.04797534325523513, -0.12917134586218773, -2.3035554199144586, -0.002636216000490143, 0.02172046075591798, -0.0026790202849454967, 0.02029159929935926, 0.0004736768509611873, -0.02331533204943195, 0.012856608906197307, -0.020195111694772845, -0.034917829983644316, -0.07707905197328785, -0.25852471463915266, -2.249225097612937, 0.006554065131679118, -0.011598944674650499, 0.0015298176249620048, 0.009554186105972744, 0.0002951877489390779, 0.01513548096690035, 0.000761197221411559, -0.017552168752613586, -0.03452766886586359, -0.08137668203524133, -0.17668998172640032, -2.2163181028900425, -0.010643287847851658, -0.002304027873737302, 0.0023562108074742763, 0.008520407548268053, 0.01343421136453065, -0.019462194037342073, 0.014245340066702356, -0.014573509321222396, -0.015949147114742324, -0.021016692564283516, -0.05961157459438526, -2.275817646629055, -0.06577250173047997]\n",
      "\n",
      "adjacency: [0.005051814050421621, 0.003732766394891706, 0.0039938157566021095, 0.004273804008559595, 0.00451341129898596, 0.0053795838501234875, 0.005944274425401968, 0.006150868516970202, 0.006825747189047936, 0.008633944616760062, 0.020049744566799713, 1.7243602805413254, 0.005502923136958118, 0.00397053284731415, 0.004315605624288467, 0.004839043112306123, 0.005056416736459148, 0.005886406107914292, 0.006229749976681064, 0.006866952399740772, 0.00880805471406079, 0.018877395370790858, 0.025630451128963876, 1.7909927705367295, 0.005558858279487548, 0.004193682972593678, 0.004631216634435496, 0.004958212051149216, 0.005732727914446574, 0.0064976070988913845, 0.006975836930750976, 0.008649585794523523, 0.020108388534784576, 0.014860362516126036, 0.04175505024807659, 1.8406611240050073, 0.006025666047896186, 0.004681528929432518, 0.005154134498931764, 0.00581456597014551, 0.0063026224243377315, 0.006808203498559984, 0.008537607102942331, 0.02000438495349826, 0.010164740351235728, 0.018114072019672266, 0.03380278716191455, 1.9482839010845305, 0.006522999889904649, 0.005248593110894428, 0.006169840219272929, 0.006437778123044016, 0.00689388459067505, 0.008591606765129309, 0.01953586993150792, 0.0078161148502446, 0.010892350602377698, 0.015907856533148985, 0.031138697163833672, 2.1059193545282695, 0.0073175916519116965, 0.005932681336286031, 0.006215997624081074, 0.006948756458464188, 0.008535972026979867, 0.019127638639498432, 0.006027382879997447, 0.008027113840024108, 0.010145535204170705, 0.014814186590726955, 0.028576879348484414, 2.364468096287364, 0.00845483163194887, 0.006468603985847666, 0.0069289649069649035, 0.00855126859844623, 0.019295434112942624, 0.005395193434487535, 0.0066980089524076225, 0.007841304736102055, 0.010052515533820404, 0.015117861621852144, 0.03361792716002528, 2.6476420839233774, 0.008743584988213088, 0.006537385164497531, 0.008816861884524201, 0.01902266802860228, 0.005141942520415869, 0.006368377548781672, 0.006982028240590198, 0.009177767240298633, 0.012401591496668257, 0.02283212601688141, 0.08078793791549248, 2.684639515231348, 0.009432035480250794, 0.008843858084869339, 0.019348050222278375, 0.004867589759690218, 0.005802208360557666, 0.006376805816898964, 0.0076834076252259974, 0.010143241534737439, 0.01608306067265219, 0.03413439268472289, 0.06422936202621748, 2.8984707663100022, 0.013589346284558436, 0.020183203184748207, 0.00444874053195485, 0.004876191528518318, 0.0053876680489700806, 0.00657087236563964, 0.007463901848897915, 0.010207098935371086, 0.01488986528283248, 0.01799882676943441, 0.027778095597722607, 3.779784676505434, 0.06321789278916813]\n",
      "\n",
      "mu: [0.30241275464168327, 0.007226954873123857, 0.00319914327148352, 0.0026615559664309235, 0.0022478764380588906, 0.0017626470077502915, 0.0013883987978285453, 0.0011043875660403228, 0.0010695207077733733, 0.0010303363647684906, 0.0008970121394837013]\n",
      "\n",
      "\n",
      "Finished.\n",
      "\n",
      "End time is:  2020-03-14 18:51:19.398314\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.optimize import fsolve\n",
    "from scipy import log\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "# External libraries\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Internal libraries\n",
    "\n",
    "import models\n",
    "import posteriors\n",
    "import priors\n",
    "import hawkes_model, excitation_kernels\n",
    "import learners\n",
    "import utils\n",
    "\n",
    "from make_data_for_samples import make_data              #多个样本数据\n",
    "from make_data_for_estimate import make_estimate_data    #单个样本数据\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "#def learn_vi(events, end_time, vi_seed, adjacency_true, inference_param_dict, return_learner=False):\n",
    "def learn_vi(events, vi_seed, inference_param_dict, return_learner=False):\n",
    "    # Extract some parameters for easier access\n",
    "    #n_nodes = len(events)\n",
    "    n_events = len(events)\n",
    "    n_nodes = len(events[0])\n",
    "    M = inference_param_dict['excitation']['args'].get('M', 1)\n",
    "    n_params = n_nodes * (n_nodes * M + 1)\n",
    "    n_edges = M * n_nodes ** 2\n",
    "    # Set seed\n",
    "    np.random.seed(vi_seed)\n",
    "    # Set starting pointM * n_nodes ** 2\n",
    "    x0 = torch.tensor(\n",
    "        np.hstack((\n",
    "            np.hstack((  # alpha, the mean of the parameters\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_nodes),\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_edges),)),\n",
    "            np.hstack((  # beta=log(sigma), log of the variance of the parameters\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_nodes), 1e-1, 2.0)),\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_edges), 1e-1, 2.0)),))\n",
    "        )),\n",
    "        dtype=torch.float64, requires_grad=True\n",
    "    )\n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **inference_param_dict['excitation'])\n",
    "    hawkes_model_obj = hawkes_model.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "    # Init the posterior object\n",
    "    posterior_obj = make_object(posteriors, **inference_param_dict['posterior'])\n",
    "    # Init the prior object\n",
    "    prior_type = inference_param_dict['prior']['name']\n",
    "    prior_args = inference_param_dict['prior']['args']\n",
    "    prior_args['C'] = torch.tensor(prior_args['C'], dtype=torch.float64)  # cast to tensor\n",
    "    prior_obj = make_object(priors, prior_type, prior_args)\n",
    "    # Init the variational inference model object\n",
    "    model = models.ModelHawkesVariational(\n",
    "        model=hawkes_model_obj, posterior=posterior_obj, prior=prior_obj,\n",
    "        **inference_param_dict['model']['args'])\n",
    "   \n",
    "    # Init the optimizer\n",
    "    opt_type = inference_param_dict['optimizer']['name']\n",
    "    opt_args = inference_param_dict['optimizer']['args']\n",
    "    opt = getattr(optim, opt_type)([x0], **opt_args)\n",
    "    # Init learner\n",
    "    learner = learners.VariationalInferenceLearner(\n",
    "        model=model, optimizer=opt, **inference_param_dict['learner']['args'])\n",
    "    # Fit the model\n",
    "    events_t = [torch.tensor(events_i) for events_i in events]  # cast to tensor\n",
    "    #learner.fit(events_t, end_time, x0, callback=callback)\n",
    "    learner.fit(events_t, x0=x0, callback=None)\n",
    "    print()\n",
    "    if return_learner:\n",
    "        return learner\n",
    "    # Extract the mode of the posterior\n",
    "    z_est_mode = learner.model.posterior.mode(learner.coeffs[:n_params], learner.coeffs[n_params:])\n",
    "    adj_est_ora = z_est_mode[n_nodes:].detach()\n",
    "    mu_est_ora = z_est_mode[:n_nodes].detach()\n",
    "    adj_est_ora = adj_est_ora.view(n_nodes, n_nodes, M)\n",
    "    adj_est = z_est_mode[n_nodes:].detach().numpy()\n",
    "    adj_est = np.reshape(adj_est, (n_nodes, n_nodes, M)).sum(-1).ravel()\n",
    "    mu_est = z_est_mode[:n_nodes].detach().numpy()\n",
    "    #mu_est = np.reshape(mu_est,n_nodes).ravel()\n",
    "    coeffs_est = learner.coeffs.detach().numpy()\n",
    "    #log_like_sum,min_intens,log_like,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    log_like_sum,intens_sum,integral_instesity,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    \n",
    "    return coeffs_est, adj_est,mu_est,intens_sum,integral_instesity,end_time\n",
    "\n",
    "\n",
    "def run(exp_dir, param_filename, output_filename, stdout=None, stderr=None):\n",
    "    # Reset random seed\n",
    "    np.random.seed(None)\n",
    "\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "\n",
    "    print('\\nExperiment parameters')\n",
    "    print('=====================')\n",
    "    print(f'        exp_dir = {exp_dir:s}')\n",
    "    print(f' param_filename = {param_filename:s}')\n",
    "    print(f'output_filename = {output_filename:s}')\n",
    "    print(flush=True)\n",
    "    print('\\nStart time is: ', datetime.datetime.today())\n",
    "\n",
    "    result_dict = {}\n",
    "    \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    global events\n",
    "    events = make_data('s052',18000,18050,data_fileName)\n",
    "    n_jumps_per_dim = list(map(len, events[0]))\n",
    "    print('\\nNumber of jumps:', len(events)*sum(n_jumps_per_dim))\n",
    "    print('\\nper node:', n_jumps_per_dim)\n",
    "    \n",
    "    C_list = [1.0]*132\n",
    "    \n",
    "    param_dict={'inference':{'vi_exp':{'excitation': {'name': 'ExponentialKernel','args': {'decay': 1.0, 'cut_off': 1000.0}}, \n",
    "                          'posterior': {'name': 'LogNormalPosterior', 'args': {}},\n",
    "                          'prior': {'name': 'GaussianLaplacianPrior', 'args': {'dim': 11, 'n_params': 132, 'C': C_list}}, \n",
    "                          'model': {'args': {'n_samples': 1, 'n_weights': 1, 'weight_temp': 1.0}}, \n",
    "                          'optimizer': {'name': 'Adam', 'args': {'lr': 0.01}}, \n",
    "                          'learner': {'args': {'tol': 1e-04, 'lr_gamma': 0.9999, 'max_iter': 40000, 'hyperparam_momentum': 0.5, 'hyperparam_interval': 100, 'hyperparam_offset': 0}}}}\n",
    "               }\n",
    "    \n",
    "\n",
    "    print('\\nINFERENCE')\n",
    "    print('=========')\n",
    "\n",
    "    for key, inference_param_dict in param_dict['inference'].items():\n",
    "        if key.startswith('vi'):\n",
    "            print(f'\\nRun VI ({key:s})')\n",
    "            print('------')\n",
    "            # Set random seed (for reproducibility)\n",
    "            np.random.seed()  # Reset random number generator to avoid dependency on simulation seed\n",
    "            #vi_seed = np.random.randint(2**32 - 1)\n",
    "            vi_seed = np.random.randint(2**16 - 1)\n",
    "            print(f'vi random seed: {vi_seed}')\n",
    "            # Run inference\n",
    "            global intens_sum\n",
    "            global integral_instesity\n",
    "            #coeffs_var, adj_var, mu_var, nu, varsigma = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            coeffs_var, adj_var, mu_var,intens_sum,integral_instesity,end_time  = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            #模型参数\n",
    "            adj_var = adj_var.ravel()\n",
    "            mu_var = mu_var.ravel()            \n",
    "          \n",
    "            global  end_time_result\n",
    "            end_time_result = [0.0]*len(end_time)\n",
    "            \n",
    "            for i in range(len(end_time)):\n",
    "                end_time_result[i]=end_time[i].tolist()\n",
    "                \n",
    "            expresion_temp = ''\n",
    "            events_n = len(events)\n",
    "            dim = len(events[0])\n",
    "            \n",
    "            for i in range(events_n):\n",
    "                temp = ''                  \n",
    "                for j in range(dim):\n",
    "                    temp += 'log('+str(intens_sum[i][j].detach().numpy())+ '-'+ 'epsilon_noise['+str(i)+'])'+ '+'   \n",
    "                #print(intens)\n",
    "                expresion_temp += temp[:-1] + '-' + str(integral_instesity[i].detach().numpy()) + '+' + str(dim) +'*'+ str(end_time_result[i])+'*'+ 'epsilon_noise['+str(i)+'],'\n",
    "\n",
    "            expresion = expresion_temp[:-1]\n",
    "            expresion = '['+expresion+']'\n",
    "         \n",
    "            result_dict.update({\n",
    "                key: {\n",
    "                    'vi_seed': vi_seed,             # VI random seed\n",
    "                    'coeffs': coeffs_var.tolist(),  # VI parameters\n",
    "                    'adjacency': adj_var.tolist(),  # VI Estimator\n",
    "                    'mu':  mu_var.tolist(),\n",
    "                    'expresion': expresion,\n",
    "                }\n",
    "            })\n",
    " \n",
    "\n",
    "    print('\\n\\nSave results...')\n",
    "    \n",
    "    print('\\ncoeffs:',  coeffs_var.tolist())\n",
    "    print( '\\nadjacency:', adj_var.tolist())\n",
    "    print('\\nmu:', mu_var.tolist())\n",
    "    #print('\\nnu:',nu)\n",
    "    #print('\\nvarsigma:',varsigma)\n",
    "\n",
    "    with open(os.path.join(exp_dir, output_filename), 'w') as output_file:\n",
    "        json.dump(result_dict, output_file)\n",
    "\n",
    "    # Log that the run is finished\n",
    "    print('\\n\\nFinished.')\n",
    "    print('\\nEnd time is: ', datetime.datetime.today())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dir', dest='dir', type=str,\n",
    "                        #required=True, help=\"Working directory\")\n",
    "                        required=False, default=\".\")\n",
    "    parser.add_argument('-p', '--params', dest='param_filename', type=str,\n",
    "                        required=False, default='params.json',\n",
    "                        help=\"Input parameter file (JSON)\")\n",
    "    parser.add_argument('-o', '--outfile', dest='output_filename', type=str,\n",
    "                        required=False, default='output.json',\n",
    "                        help=\"Output file (JSON)\")\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    #run(exp_dir=args.dir, param_filename=args.param_filename,output_filename=args.output_filename)\n",
    "    run ('.','params.json','penalty10_decay1.0+s052_18000-18050.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
