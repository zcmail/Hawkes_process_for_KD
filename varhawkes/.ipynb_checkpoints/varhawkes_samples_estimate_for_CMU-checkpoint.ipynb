{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start time is:  2020-03-10 22:15:49.844273\n",
      "events num is: 2500\n",
      "mean: -4.094931164390759\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d191c83e5359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    249\u001b[0m                         help=\"Input parameter file (JSON)\")\n\u001b[0;32m    250\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     \u001b[0mestimate\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./result/2/penalty10+decay0.3+1e-04+c1+epsilon=-3.0/penalty10_decay0.3+s002_0-0_200.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d191c83e5359>\u001b[0m in \u001b[0;36mestimate\u001b[1;34m(exp_dir, param_filename, stdout, stderr)\u001b[0m\n\u001b[0;32m    210\u001b[0m                                       threshold=threshold)\n\u001b[0;32m    211\u001b[0m         out_num_all = decision_fun_2(mu=mu, W=W, new_events=all_events, M=1 ,param_dict=param_dict_exitation,nu=nu, sigma=sigma,\n\u001b[1;32m--> 212\u001b[1;33m                                      threshold=threshold, mean=mean)\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mFN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d191c83e5359>\u001b[0m in \u001b[0;36mdecision_fun_2\u001b[1;34m(mu, W, new_events, M, param_dict, nu, sigma, threshold, mean)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevents_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mloglik\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mhawkes_model_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_events\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mloglik_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhawkes_model_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mloglik_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhawkes_model_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\project\\time_series\\var-hawkes-master-new-modify-penalty\\varhawkes\\hawkes_model_single.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, events)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#if not self._fitted:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                        \u001b[1;31m#执行_init_cache()\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;31m#self._fitted = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\project\\time_series\\var-hawkes-master-new-modify-penalty\\varhawkes\\hawkes_model_single.py\u001b[0m in \u001b[0;36m_init_cache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m                     self.events[i].cpu().numpy() - self.excitation.cut_off)  #cut_off是时间序列截断（事件序列考虑多长一段，可以认为是最大时间）。\n\u001b[0;32m     68\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                     \u001b[0mt_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_i\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_start\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mid_end\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;31m#如果cut_off是0的话，那么events[j][id_start[k]:id_end[k]]为空。\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                     \u001b[0mkappas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcitation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_ij\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m              \u001b[1;31m#如果是高斯混合模型的话，要加上tensor所有的值。其它情况就一个值。\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkappas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# Internal libraries\n",
    "import excitation_kernels\n",
    "import hawkes_model_single\n",
    "\n",
    "\n",
    "from make_data_for_samples import make_data #多个样本数据\n",
    "from make_data_for_samples import make_data_all_expect #取其他用户的前50个样本作为入侵者\n",
    "from make_data_for_estimate import make_estimate_data     #单个样本数据\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global result\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "param_dict_exitation = {'exitation':{'name': 'ExponentialKernel', 'args': {'decay':0.3, 'cut_off': 1000.0}}}\n",
    "\n",
    "#求正样本的情况\n",
    "def decision_fun_1(mu,W, new_events, M ,param_dict, nu, sigma, threshold):\n",
    "    events_num = len(new_events)\n",
    "    dim = len(new_events[0])\n",
    "    n_params = dim * (M * dim + 1)\n",
    "    #3倍上界\n",
    "    up =  math.exp(nu) * (math.exp(sigma))**3\n",
    "    #3倍下界\n",
    "    down = math.exp(nu) / (math.exp(sigma))**3    \n",
    "    \n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **param_dict['exitation'])\n",
    "    hawkes_model_obj = hawkes_model_single.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "   \n",
    "    W = W.view(dim,dim,M)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    loglik_all=[]\n",
    "\n",
    "    for i in range(events_num):\n",
    "        loglik=[0.0,0.0]        \n",
    "        hawkes_model_obj.set_data(new_events[i])\n",
    "        loglik_max = hawkes_model_obj.log_likelihood(mu,W,epsilon_noise=up)\n",
    "        loglik_min = hawkes_model_obj.log_likelihood(mu,W,epsilon_noise=down)\n",
    "        \n",
    "        loglik[0] = loglik_min.item()\n",
    "        loglik[1] = loglik_max.item()\n",
    "        \n",
    "        if math.isnan(loglik[0]):\n",
    "            loglik[0] = loglik_all[i-1][0]\n",
    "        if math.isnan(loglik[1]):\n",
    "            loglik[1] = loglik_all[i-1][1]\n",
    "        \n",
    "        loglik_all.append(loglik)\n",
    "        \n",
    "    loglik_list = sum(loglik_all,[])\n",
    "    #print(loglik_list)\n",
    "    \n",
    "    mean = np.mean(loglik_list)\n",
    "    print('mean:',mean)\n",
    "    \n",
    "    distance = list( map(lambda x: abs(x - mean), loglik_list) )\n",
    "    \n",
    "    distance_array = np.array(distance)\n",
    "    #距离大于threshold的正样本个数\n",
    "    out_num = np.sum((distance_array > threshold))  \n",
    "    \n",
    "    return mean,out_num\n",
    "\n",
    "#求所有样本的情况\n",
    "def decision_fun_2(mu,W, new_events, M ,param_dict, nu, sigma,threshold, mean):\n",
    "    events_num = len(new_events)\n",
    "    dim = len(new_events[0])\n",
    "    n_params = dim * (M * dim + 1)\n",
    "    #3倍上界\n",
    "    up =  math.exp(nu) * (math.exp(sigma))**3\n",
    "    #3倍下界\n",
    "    down = math.exp(nu) / (math.exp(sigma))**3    \n",
    "    \n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **param_dict['exitation'])\n",
    "    hawkes_model_obj = hawkes_model_single.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "   \n",
    "    W = W.view(dim,dim,M)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    loglik_all=[]\n",
    "\n",
    "    for i in range(events_num):\n",
    "        loglik=[0.0,0.0]        \n",
    "        hawkes_model_obj.set_data(new_events[i])\n",
    "        loglik_max = hawkes_model_obj.log_likelihood(mu,W,epsilon_noise=up)\n",
    "        loglik_min = hawkes_model_obj.log_likelihood(mu,W,epsilon_noise=down)\n",
    "        \n",
    "        loglik[0] = loglik_min.item()\n",
    "        loglik[1] = loglik_max.item()\n",
    "        \n",
    "        if math.isnan(loglik[0]):\n",
    "            loglik[0] = loglik_all[i-1][0]\n",
    "        if math.isnan(loglik[1]):\n",
    "            loglik[1] = loglik_all[i-1][1]\n",
    "            \n",
    "        loglik_all.append(loglik)\n",
    "        \n",
    "    loglik_list = sum(loglik_all,[])\n",
    "  \n",
    "    distance = list( map(lambda x: abs(x - mean), loglik_list) )\n",
    "    \n",
    "    distance_array = np.array(distance)\n",
    "    #距离大于threshold的正样本个数\n",
    "    out_num = np.sum((distance_array > threshold))  \n",
    "    \n",
    "    return out_num\n",
    "\n",
    "def estimate(exp_dir, param_filename, stdout=None, stderr=None):\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "    print('\\nStart time is: ', datetime.datetime.today())\n",
    "   \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    \n",
    "    user = 's002'\n",
    "    \n",
    "    if user == 's036':\n",
    "        events = make_data('s036',12000,12400,data_fileName)\n",
    "    if user == 's047':\n",
    "        events = make_data('s047',16000,16400,data_fileName)\n",
    "    if user == 's052':\n",
    "        events = make_data('s052',18000,18400,data_fileName)\n",
    "    if user == 's032':\n",
    "        events = make_data('s032',10400,10800,data_fileName)\n",
    "    if user == 's010':\n",
    "        events = make_data('s010',2400,2800,data_fileName)\n",
    "    if user == 's008':\n",
    "        events = make_data('s008',2000,2400,data_fileName)\n",
    "    if user == 's007':\n",
    "        events = make_data('s007',1600,2000,data_fileName)\n",
    "    if user == 's005':\n",
    "        events = make_data('s005',1200,1600,data_fileName)\n",
    "    if user == 's004':\n",
    "        events = make_data('s004',800,1200,data_fileName)\n",
    "    if user == 's003':\n",
    "        events = make_data('s003',400,800,data_fileName)\n",
    "    if user == 's002':\n",
    "        events = make_data('s002',0,400,data_fileName)\n",
    "        \n",
    "    n_jumps_per_dim = list(map(len, events[0]))\n",
    "    n_nodes = len(events[0])\n",
    "    #print('Number of jumps:', len(events)*sum(n_jumps_per_dim))\n",
    "    #print('per node:', n_jumps_per_dim)\n",
    "    \n",
    "    events = torch.tensor(events, dtype=torch.float32)\n",
    "    \n",
    "    #print('\\nestimating')\n",
    "    #print('=========')\n",
    "\n",
    "    param_filename = os.path.join(exp_dir, param_filename)\n",
    "    if not os.path.exists(param_filename):\n",
    "        raise FileNotFoundError(\n",
    "            'Input file `{:s}` not found.'.format(param_filename))\n",
    "    with open(param_filename, 'r') as param_file:\n",
    "        param_dict = json.load(param_file)\n",
    "        \n",
    "    mu = torch.tensor(param_dict['vi_exp']['mu'],dtype=torch.float32)\n",
    "    W = torch.tensor(param_dict['vi_exp']['adjacency'],dtype=torch.float32)\n",
    "    \n",
    "    #W = W.view(n_nodes,n_nodes)\n",
    "    \n",
    "    #print('mu:',mu)\n",
    "    #print('W:',W)\n",
    "    \n",
    "    #nu = -1.3839905543829933\n",
    "    #sigma = 0.2960047784292162\n",
    "    \n",
    "    nu = param_dict['vi_exp']['nu']\n",
    "    sigma = param_dict['vi_exp']['sigma']\n",
    "    \n",
    "    #threshold_range = np.linspace(1,6,20)\n",
    "    threshold_range = np.linspace(0.1,6,40)\n",
    "    FN = [0]*len(threshold_range)\n",
    "    TP = [0]*len(threshold_range)\n",
    "    FP = [0]*len(threshold_range)\n",
    "    TN = [0]*len(threshold_range)\n",
    "    \n",
    "    false_alarm_rate = [0.0]*len(threshold_range)\n",
    "    miss_rate = [0.0]*len(threshold_range)\n",
    "    recall = [0.0]*len(threshold_range)\n",
    "    precision = [0.0]*len(threshold_range)\n",
    "    #hit_rate = [0.0]*len(threshold_range)\n",
    "    \n",
    "    all_events = make_data_all_expect(data_fileName,user)\n",
    "    print('events num is:',len(all_events))\n",
    "    \n",
    "    all_events = torch.tensor(all_events, dtype=torch.float32)\n",
    "    \n",
    "    for i,threshold in enumerate(threshold_range):\n",
    "        mean,out_num = decision_fun_1(mu=mu, W=W, new_events=events, M=1 ,param_dict=param_dict_exitation,nu=nu, sigma=sigma,\n",
    "                                      threshold=threshold)\n",
    "        out_num_all = decision_fun_2(mu=mu, W=W, new_events=all_events, M=1 ,param_dict=param_dict_exitation,nu=nu, sigma=sigma,\n",
    "                                     threshold=threshold, mean=mean)\n",
    "        \n",
    "        FN[i] = out_num\n",
    "        TP[i] = 800-out_num                         #400*2-out_num\n",
    "        FP[i] = 5000 -  out_num_all                 #2500*2 - out_num_all\n",
    "        TN[i] = out_num_all\n",
    "        \n",
    "        false_alarm_rate[i] = FP[i]/(FP[i]+TN[i])\n",
    "        miss_rate[i] = FN[i]/(TP[i]+FN[i])\n",
    "        recall[i] = TP[i]/(TP[i]+FN[i])\n",
    "        precision[i] = TP[i]/(TP[i]+FP[i])\n",
    "    \n",
    "    print('FN is:', FN)\n",
    "    print('TP is:', TP)\n",
    "    print('FP is:', FP)\n",
    "    print('TN is:', TN)\n",
    "    \n",
    "    print('false_alarm_rate is:',false_alarm_rate)\n",
    "    print('miss_rate is:',miss_rate)\n",
    "    \n",
    "    #画false-alarm/hit rate ROC图\n",
    "    hit_rate = list( map(lambda x: 1 - x, miss_rate) )\n",
    "    print('hit_rate is:',hit_rate)\n",
    "    plt.plot(false_alarm_rate,hit_rate,color=\"blue\",linewidth=1)\n",
    "    plt.plot(false_alarm_rate,false_alarm_rate,color=\"red\",linewidth=1)\n",
    "  \n",
    "    print('\\n\\nFinished.')\n",
    "    print('\\nEnd time is: ', datetime.datetime.today())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dir', dest='dir', type=str,\n",
    "                        #required=True, help=\"Working directory\")\n",
    "                        required=False, default=\".\")\n",
    "    parser.add_argument('-p', '--params', dest='param_filename', type=str,\n",
    "                        required=False, default='params.json',\n",
    "                        help=\"Input parameter file (JSON)\")\n",
    "    args = parser.parse_known_args()[0]\n",
    "    estimate ('.','./result/2/penalty10+decay0.3+1e-04+c1+epsilon=-3.0/penalty10_decay0.3+s002_0-0_200.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
