{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment parameters\n",
      "=====================\n",
      "        exp_dir = .\n",
      " param_filename = params.json\n",
      "output_filename = penalty10_decay0.4+s036_12350-12400.json\n",
      "\n",
      "\n",
      "Start time is:  2020-03-06 16:22:26.227643\n",
      "\n",
      "Number of jumps: 1100\n",
      "\n",
      "per node: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "INFERENCE\n",
      "=========\n",
      "\n",
      "Run VI (vi_exp)\n",
      "------\n",
      "vi random seed: 59793\n",
      "end_iter is: 39837\n",
      "Converged!\n",
      "\n",
      "\n",
      "\n",
      "Save results...\n",
      "\n",
      "coeffs: [-0.9027220884442534, -5.247355493365638, -5.514135168890337, -5.440967015536157, -5.408941316113923, -5.260886774177557, -5.500570870026907, -5.728112855810781, -5.776661753102691, -5.794373032454098, -5.838068207292654, -4.302391205853811, -5.073677963700025, -5.054975541873333, -5.016608385247834, -4.958979934919844, -4.854162627135836, -4.706252589450889, -4.610221150896877, -4.497755736999033, -4.201514409704477, -2.973861678706331, 0.07704705413575201, -4.645616422244368, -5.036045711396284, -5.022608133459012, -4.973865104246858, -4.842140948482446, -4.71646474063263, -4.6258131899120984, -4.501829722274123, -4.217290191653603, -2.985496325036621, -3.755817882175209, 0.07974411473978407, -4.728303904337961, -5.022033551811434, -4.9591293092703435, -4.860085121416285, -4.715368528673669, -4.6191396883902485, -4.500345168120965, -4.188162227939576, -3.0109110890695066, -4.0182417680127225, -2.9167136010458976, 0.037727514364132596, -4.664197066759725, -4.96633757754206, -4.852853885141555, -4.708752987169211, -4.621437775225953, -4.498607724265264, -4.206226848326419, -2.998812698881184, -4.291382872047126, -3.7174497383235137, -3.3897457184131436, 0.06785522741620581, -4.592403223814321, -4.853877935160376, -4.707140833887015, -4.639822338494811, -4.509288494027231, -4.208396350289971, -3.00750216499874, -4.530694415521293, -4.19889413146674, -4.026697074085737, -3.552345138345183, 0.1350023030108242, -4.350145663794636, -4.714268188677492, -4.630943300371969, -4.5101110112537635, -4.215157110855367, -3.0141577006036657, -4.777305237203842, -4.590887195306189, -4.503511297581377, -4.321071830157619, -3.948278023712289, 0.25367917529414324, -4.223292169257922, -4.619281276229691, -4.510576644324884, -4.2062593853289165, -3.0016171138453496, -4.881091403773259, -4.768967590625804, -4.729569895980613, -4.633636608837571, -4.442556130648317, -3.864068110113425, 0.40897683777242655, -4.262688496093673, -4.500205699274744, -4.213633460187411, -3.0160283126244867, -4.947205764940638, -4.842262090343184, -4.803292132536891, -4.724811329193636, -4.584710635744564, -4.126143437381888, -3.120851121562682, 0.4683010778853738, -4.120262941937771, -4.21708517785913, -2.9960011517681093, -4.967406584154788, -4.894106389104049, -4.867954298218641, -4.795857515452505, -4.677628911552048, -4.335359409922497, -3.716890081340142, -3.117287259028278, 0.5730844438094964, -3.667440861332035, -2.995431533319549, -5.017090013990018, -4.959095109422772, -4.946325136935338, -4.894756353505026, -4.786422946640695, -4.580488164123333, -4.242461457267026, -4.000450740749679, -3.602864274184165, 0.7520027476110236, 0.07916456278881401, -2.42825699892923, -0.0014188424206283857, -0.008232691035244816, -0.002650070874232, -0.001048219373915702, -0.008515687432595532, -0.015899125606643076, 0.01379969052367944, 0.01989731365169139, 0.005128255321646828, -0.001931518900858488, -0.02700218532087368, 0.002917289381893046, 0.01817923136607426, 0.009316870928469511, 0.014567735510486103, 0.017731781411305922, 0.008912070034078493, 0.021341781046002987, 0.005998412689568936, 0.011494124678954486, 0.03419931249671004, -2.34810907608124, 0.01455964656736552, 0.007543607344378948, -0.008694118724779185, 0.0018860893168571762, 0.019455522429570993, 0.0029656870142392676, 0.0014629469295076147, 0.007041470495003636, 0.025915044318640983, 0.004759177347716926, -0.06057534162222365, -2.3346725719971664, 0.006571628775238139, -0.0014270830675466564, 0.018207996302454236, 0.009306656465442783, 0.0007303260592963315, -0.013437419109416844, 0.0072730917499745085, 0.023424254292254035, 0.0028376385604755396, -0.030776396885656428, -0.24863177761787808, -2.281736153135534, 0.0159536823553366, 0.0067077563810976236, 0.008390386305486101, 0.028860789546622412, -0.0021597895114461303, 0.007672096758425748, 0.017833699171800117, 0.009345701075320732, -0.007115037778820713, -0.03721745482264222, -0.11568355746233087, -2.282634450763928, -0.01923614517899713, 0.01596128515076859, 0.01768634102906292, -0.0009174297520277594, -0.006203852441321676, 0.011815504771806727, -0.00815198874997188, -0.0010627528997237113, -0.0005665538542569929, -0.011230902959772625, -0.09416347517506399, -2.286499953684764, -0.004812826334677548, -0.0010706694564124322, 0.021082640811429833, 0.007788391672649025, 0.004280109920235141, 0.013506713731715572, -0.015541423776038647, 0.016141269639415, -0.012569521232848114, -0.006833312131087391, -0.01580625687878592, -2.3067518656897623, -0.007336410822124031, 0.01724339556051179, 0.008658048214656712, 0.0033051431030922044, -0.004756023928414538, 0.025210817730389294, 0.011599250814759711, 0.001173861667767432, -0.003642589407641267, -0.008931817952195505, -0.021159425916146998, -2.3027258883504227, -0.011455401881506934, 0.006053786110016114, 0.008922638349146373, -0.0014262552259907413, 0.018255871739233882, -0.013115314865052305, -0.009407903149441777, -0.000807309023171943, -0.005073484629954628, 0.0013168031907487708, -0.10326075906600911, -2.2626810660790513, -0.005658874761308648, -0.013376894295892021, 0.02782157101483503, 0.009968760967534473, -0.0031901927457068974, 0.006612555709335127, 0.01657823977502424, 0.015333033913252905, 0.0037141765123162784, -0.04249433995387011, -0.08768664049565378, -2.265427633383526, -0.011105838362763971, 0.0071469272938069, 0.009814580838493285, 0.0006570483766587687, 0.014926335596080403, 0.003437723849797611, 0.006617153077924876, -0.0033624682368460206, -0.0027760836359815204, -0.013182180981539228, -0.011973263980237766, -2.1844254321801966, -0.8133567920924913]\n",
      "\n",
      "adjacency: [0.0052484673859752385, 0.002289253271811825, 0.002260875712231191, 0.002392499081278398, 0.0025073120500742735, 0.0027662503999405304, 0.0032658457077503283, 0.0035040748091776184, 0.004046816271034371, 0.0053816033687180236, 0.017515768500282317, 1.07027680893491, 0.0034301069849903567, 0.0023549228488049393, 0.0024654764627694468, 0.0025347902005179953, 0.0027897063395879093, 0.0032718040790004964, 0.003593087679160514, 0.004021853974291015, 0.005141106404610069, 0.018406339726855597, 0.009641116656276873, 1.0729004753540339, 0.0032098408768283133, 0.002431654339945792, 0.0024881483326843236, 0.002797937495846105, 0.003290123265912087, 0.0037252445989241724, 0.004025936750309011, 0.005320832688090348, 0.01801406385138535, 0.007023138575839338, 0.02945456797604496, 1.0276778268384332, 0.00335729755096854, 0.0025292221298868016, 0.00282350503446682, 0.003125462592530607, 0.003635067814576917, 0.004029672207772402, 0.005286822058594431, 0.017994666352268915, 0.005106436043944747, 0.009602636030378395, 0.015249724459445398, 1.0591303502142226, 0.003869377676336013, 0.002777190438238165, 0.0032046686749639245, 0.0035600198392298408, 0.004099224590868495, 0.0053411794236206765, 0.01847511887209821, 0.003971659875479205, 0.005528926167023042, 0.006707790570919802, 0.01251673194685724, 1.1327806583915279, 0.00479315610752747, 0.003305626901887842, 0.003434066454883935, 0.003982642461015224, 0.005387067667269316, 0.017570405491811374, 0.0031932997420020272, 0.0036112674698439576, 0.004174813428167071, 0.004954306056154042, 0.007319890124135103, 1.2760408065559095, 0.005468633729928963, 0.0035021800528136837, 0.0039737560093646975, 0.005445903699416214, 0.018459979463103095, 0.0026510331506814752, 0.0030505381222227547, 0.003240847603486146, 0.0036015970654146106, 0.004405722108959741, 0.008045581646376313, 1.4903032955197038, 0.005300054624908308, 0.0040364613881134544, 0.005344758514500907, 0.018075825038150646, 0.002517743414511234, 0.00297838969529618, 0.0030743793408095636, 0.0032692390945714663, 0.0037929356825721996, 0.005923780033082272, 0.019560170849154124, 1.5800717957223782, 0.006042067663595601, 0.005568185163686762, 0.017366338929773364, 0.002509820139521515, 0.002773219622802836, 0.0027912458315187965, 0.002939341222053156, 0.003316749550316673, 0.004782387301929291, 0.00970210153889691, 0.019131272390171882, 1.7547264646094731, 0.009604975364682218, 0.018136513840895534, 0.002388921919761037, 0.0025788542444635256, 0.002537368226429763, 0.002734913443407823, 0.0030283262227234157, 0.0037960820279038647, 0.005316581938973436, 0.0069124518045097175, 0.010263081026629671, 2.094546287475162, 0.8892205587829943]\n",
      "\n",
      "mu: [0.40232315799590385, 0.0019410585100926511, 0.001506743468720724, 0.0016033166407570835, 0.001650219328798674, 0.0019420724298276914, 0.0015503526359757808, 0.0011637629193577803, 0.001094717658448087, 0.0011085722848917538, 0.0010763144486402264]\n",
      "\n",
      "\n",
      "Finished.\n",
      "\n",
      "End time is:  2020-03-06 22:17:34.136875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.optimize import fsolve\n",
    "from scipy import log\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "# External libraries\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Internal libraries\n",
    "\n",
    "import models\n",
    "import posteriors\n",
    "import priors\n",
    "import hawkes_model, excitation_kernels\n",
    "import learners\n",
    "import utils\n",
    "\n",
    "from make_data_for_samples import make_data              #多个样本数据\n",
    "from make_data_for_estimate import make_estimate_data    #单个样本数据\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "#def learn_vi(events, end_time, vi_seed, adjacency_true, inference_param_dict, return_learner=False):\n",
    "def learn_vi(events, vi_seed, inference_param_dict, return_learner=False):\n",
    "    # Extract some parameters for easier access\n",
    "    #n_nodes = len(events)\n",
    "    n_events = len(events)\n",
    "    n_nodes = len(events[0])\n",
    "    M = inference_param_dict['excitation']['args'].get('M', 1)\n",
    "    n_params = n_nodes * (n_nodes * M + 1)\n",
    "    n_edges = M * n_nodes ** 2\n",
    "    # Set seed\n",
    "    np.random.seed(vi_seed)\n",
    "    # Set starting pointM * n_nodes ** 2\n",
    "    x0 = torch.tensor(\n",
    "        np.hstack((\n",
    "            np.hstack((  # alpha, the mean of the parameters\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_nodes),\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_edges),)),\n",
    "            np.hstack((  # beta=log(sigma), log of the variance of the parameters\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_nodes), 1e-1, 2.0)),\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_edges), 1e-1, 2.0)),))\n",
    "        )),\n",
    "        dtype=torch.float64, requires_grad=True\n",
    "    )\n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **inference_param_dict['excitation'])\n",
    "    hawkes_model_obj = hawkes_model.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "    # Init the posterior object\n",
    "    posterior_obj = make_object(posteriors, **inference_param_dict['posterior'])\n",
    "    # Init the prior object\n",
    "    prior_type = inference_param_dict['prior']['name']\n",
    "    prior_args = inference_param_dict['prior']['args']\n",
    "    prior_args['C'] = torch.tensor(prior_args['C'], dtype=torch.float64)  # cast to tensor\n",
    "    prior_obj = make_object(priors, prior_type, prior_args)\n",
    "    # Init the variational inference model object\n",
    "    model = models.ModelHawkesVariational(\n",
    "        model=hawkes_model_obj, posterior=posterior_obj, prior=prior_obj,\n",
    "        **inference_param_dict['model']['args'])\n",
    "   \n",
    "    # Init the optimizer\n",
    "    opt_type = inference_param_dict['optimizer']['name']\n",
    "    opt_args = inference_param_dict['optimizer']['args']\n",
    "    opt = getattr(optim, opt_type)([x0], **opt_args)\n",
    "    # Init learner\n",
    "    learner = learners.VariationalInferenceLearner(\n",
    "        model=model, optimizer=opt, **inference_param_dict['learner']['args'])\n",
    "    # Fit the model\n",
    "    events_t = [torch.tensor(events_i) for events_i in events]  # cast to tensor\n",
    "    #learner.fit(events_t, end_time, x0, callback=callback)\n",
    "    learner.fit(events_t, x0=x0, callback=None)\n",
    "    print()\n",
    "    if return_learner:\n",
    "        return learner\n",
    "    # Extract the mode of the posterior\n",
    "    z_est_mode = learner.model.posterior.mode(learner.coeffs[:n_params], learner.coeffs[n_params:])\n",
    "    adj_est_ora = z_est_mode[n_nodes:].detach()\n",
    "    mu_est_ora = z_est_mode[:n_nodes].detach()\n",
    "    adj_est_ora = adj_est_ora.view(n_nodes, n_nodes, M)\n",
    "    adj_est = z_est_mode[n_nodes:].detach().numpy()\n",
    "    adj_est = np.reshape(adj_est, (n_nodes, n_nodes, M)).sum(-1).ravel()\n",
    "    mu_est = z_est_mode[:n_nodes].detach().numpy()\n",
    "    #mu_est = np.reshape(mu_est,n_nodes).ravel()\n",
    "    coeffs_est = learner.coeffs.detach().numpy()\n",
    "    #log_like_sum,min_intens,log_like,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    log_like_sum,intens_sum,integral_instesity,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    \n",
    "    return coeffs_est, adj_est,mu_est,intens_sum,integral_instesity,end_time\n",
    "\n",
    "\n",
    "def run(exp_dir, param_filename, output_filename, stdout=None, stderr=None):\n",
    "    # Reset random seed\n",
    "    np.random.seed(None)\n",
    "\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "\n",
    "    print('\\nExperiment parameters')\n",
    "    print('=====================')\n",
    "    print(f'        exp_dir = {exp_dir:s}')\n",
    "    print(f' param_filename = {param_filename:s}')\n",
    "    print(f'output_filename = {output_filename:s}')\n",
    "    print(flush=True)\n",
    "    print('\\nStart time is: ', datetime.datetime.today())\n",
    "\n",
    "    result_dict = {}\n",
    "    \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    global events\n",
    "    events = make_data('s002',200,250,data_fileName)\n",
    "    n_jumps_per_dim = list(map(len, events[0]))\n",
    "    print('\\nNumber of jumps:', len(events)*sum(n_jumps_per_dim))\n",
    "    print('\\nper node:', n_jumps_per_dim)\n",
    "    \n",
    "    C_list = [1.0]*132\n",
    "    \n",
    "    param_dict={'inference':{'vi_exp':{'excitation': {'name': 'ExponentialKernel','args': {'decay': 0.3, 'cut_off': 1000.0}}, \n",
    "                          'posterior': {'name': 'LogNormalPosterior', 'args': {}},\n",
    "                          'prior': {'name': 'GaussianLaplacianPrior', 'args': {'dim': 11, 'n_params': 132, 'C': C_list}}, \n",
    "                          'model': {'args': {'n_samples': 1, 'n_weights': 1, 'weight_temp': 1.0}}, \n",
    "                          'optimizer': {'name': 'Adam', 'args': {'lr': 0.01}}, \n",
    "                          'learner': {'args': {'tol': 1e-04, 'lr_gamma': 0.9999, 'max_iter': 40000, 'hyperparam_momentum': 0.5, 'hyperparam_interval': 100, 'hyperparam_offset': 0}}}}\n",
    "               }\n",
    "    \n",
    "\n",
    "    print('\\nINFERENCE')\n",
    "    print('=========')\n",
    "\n",
    "    for key, inference_param_dict in param_dict['inference'].items():\n",
    "        if key.startswith('vi'):\n",
    "            print(f'\\nRun VI ({key:s})')\n",
    "            print('------')\n",
    "            # Set random seed (for reproducibility)\n",
    "            np.random.seed()  # Reset random number generator to avoid dependency on simulation seed\n",
    "            #vi_seed = np.random.randint(2**32 - 1)\n",
    "            vi_seed = np.random.randint(2**16 - 1)\n",
    "            print(f'vi random seed: {vi_seed}')\n",
    "            # Run inference\n",
    "            global intens_sum\n",
    "            global integral_instesity\n",
    "            #coeffs_var, adj_var, mu_var, nu, varsigma = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            coeffs_var, adj_var, mu_var,intens_sum,integral_instesity,end_time  = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            #模型参数\n",
    "            adj_var = adj_var.ravel()\n",
    "            mu_var = mu_var.ravel()            \n",
    "          \n",
    "            global  end_time_result\n",
    "            end_time_result = [0.0]*len(end_time)\n",
    "            \n",
    "            for i in range(len(end_time)):\n",
    "                end_time_result[i]=end_time[i].tolist()\n",
    "                \n",
    "            expresion_temp = ''\n",
    "            events_n = len(events)\n",
    "            dim = len(events[0])\n",
    "            \n",
    "            for i in range(events_n):\n",
    "                temp = ''                  \n",
    "                for j in range(dim):\n",
    "                    temp += 'log('+str(intens_sum[i][j].detach().numpy())+ '-'+ 'epsilon_noise['+str(i)+'])'+ '+'   \n",
    "                #print(intens)\n",
    "                expresion_temp += temp[:-1] + '-' + str(integral_instesity[i].detach().numpy()) + '+' + str(dim) +'*'+ str(end_time_result[i])+'*'+ 'epsilon_noise['+str(i)+'],'\n",
    "\n",
    "            expresion = expresion_temp[:-1]\n",
    "            expresion = '['+expresion+']'\n",
    "         \n",
    "            result_dict.update({\n",
    "                key: {\n",
    "                    'vi_seed': vi_seed,             # VI random seed\n",
    "                    'coeffs': coeffs_var.tolist(),  # VI parameters\n",
    "                    'adjacency': adj_var.tolist(),  # VI Estimator\n",
    "                    'mu':  mu_var.tolist(),\n",
    "                    'expresion': expresion,\n",
    "                }\n",
    "            })\n",
    " \n",
    "\n",
    "    print('\\n\\nSave results...')\n",
    "    \n",
    "    print('\\ncoeffs:',  coeffs_var.tolist())\n",
    "    print( '\\nadjacency:', adj_var.tolist())\n",
    "    print('\\nmu:', mu_var.tolist())\n",
    "    #print('\\nnu:',nu)\n",
    "    #print('\\nvarsigma:',varsigma)\n",
    "\n",
    "    with open(os.path.join(exp_dir, output_filename), 'w') as output_file:\n",
    "        json.dump(result_dict, output_file)\n",
    "\n",
    "    # Log that the run is finished\n",
    "    print('\\n\\nFinished.')\n",
    "    print('\\nEnd time is: ', datetime.datetime.today())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dir', dest='dir', type=str,\n",
    "                        #required=True, help=\"Working directory\")\n",
    "                        required=False, default=\".\")\n",
    "    parser.add_argument('-p', '--params', dest='param_filename', type=str,\n",
    "                        required=False, default='params.json',\n",
    "                        help=\"Input parameter file (JSON)\")\n",
    "    parser.add_argument('-o', '--outfile', dest='output_filename', type=str,\n",
    "                        required=False, default='output.json',\n",
    "                        help=\"Output file (JSON)\")\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    #run(exp_dir=args.dir, param_filename=args.param_filename,output_filename=args.output_filename)\n",
    "    run ('.','params.json','penalty10_decay0.3+s002_200-250.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
