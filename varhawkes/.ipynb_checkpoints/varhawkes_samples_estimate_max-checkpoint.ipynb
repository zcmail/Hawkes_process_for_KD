{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is: 0\n",
      "count is: 31\n",
      "i is: 1\n",
      "count is: 30\n",
      "i is: 2\n",
      "count is: 30\n",
      "i is: 3\n",
      "count is: 29\n",
      "i is: 4\n",
      "count is: 29\n",
      "i is: 5\n",
      "count is: 29\n",
      "i is: 6\n",
      "count is: 29\n",
      "i is: 7\n",
      "count is: 29\n",
      "i is: 8\n",
      "count is: 29\n",
      "i is: 9\n",
      "count is: 29\n",
      "i is: 10\n",
      "count is: 29\n",
      "i is: 11\n",
      "count is: 28\n",
      "i is: 12\n",
      "count is: 27\n",
      "i is: 13\n",
      "count is: 27\n",
      "i is: 14\n",
      "count is: 27\n",
      "i is: 15\n",
      "count is: 27\n",
      "i is: 16\n",
      "count is: 27\n",
      "i is: 17\n",
      "count is: 27\n",
      "i is: 18\n",
      "count is: 27\n",
      "i is: 19\n",
      "count is: 27\n",
      "i is: 20\n",
      "count is: 27\n",
      "i is: 21\n",
      "count is: 27\n",
      "i is: 22\n",
      "count is: 27\n",
      "i is: 23\n",
      "count is: 27\n",
      "i is: 24\n",
      "count is: 27\n",
      "i is: 25\n",
      "count is: 27\n",
      "i is: 26\n",
      "count is: 26\n",
      "i is: 27\n",
      "count is: 25\n",
      "i is: 28\n",
      "count is: 24\n",
      "i is: 29\n",
      "count is: 24\n",
      "i is: 30\n",
      "count is: 24\n",
      "i is: 31\n",
      "count is: 24\n",
      "i is: 32\n",
      "count is: 24\n",
      "i is: 33\n",
      "count is: 24\n",
      "i is: 34\n",
      "count is: 23\n",
      "i is: 35\n",
      "count is: 22\n",
      "i is: 36\n",
      "count is: 22\n",
      "i is: 37\n",
      "count is: 22\n",
      "i is: 38\n",
      "count is: 22\n",
      "i is: 39\n",
      "count is: 22\n",
      "count is 20\n",
      "false alarm is: 0.05\n",
      "threshold is: -8.799999999999983\n",
      "miss rate: 0.977721756463622\n",
      "\n",
      "\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Internal libraries\n",
    "import excitation_kernels\n",
    "import hawkes_model_single\n",
    "\n",
    "\n",
    "from make_data_for_samples import make_data #多个样本数据\n",
    "from make_data_for_samples import make_data_all\n",
    "from make_data_for_estimate import make_estimate_data     #单个样本数据\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global result\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "param_dict_exitation = {'exitation':{'name': 'ExponentialKernel', 'args': {'decay':0.3, 'cut_off': 1000.0}}}\n",
    "\n",
    "def decision_fun(mu,W, new_events, M ,param_dict, nu, sigma,threshold):\n",
    "    events_num = len(new_events)\n",
    "    dim = len(new_events[0])\n",
    "    n_params = dim * (M * dim + 1)\n",
    "    #3倍上界\n",
    "    up =  math.exp(nu) * (math.exp(sigma))**3\n",
    "    #3倍下界\n",
    "    down = math.exp(nu) / (math.exp(sigma))**3    \n",
    "    \n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **param_dict['exitation'])\n",
    "    hawkes_model_obj = hawkes_model_single.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "   \n",
    "    W = W.view(dim,dim,M)\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for i in range(events_num):\n",
    "        hawkes_model_obj.set_data(new_events[i])\n",
    "        loglik_max = hawkes_model_obj.log_likelihood(mu,W,epsilon_noise=up)\n",
    "        #loglik_min = hawkes_model_obj.log_likelihood(mu,W,epsilon_noise=down)\n",
    "        '''\n",
    "        if loglik[0]<-6.0:\n",
    "            print('\\nmin is:',i)\n",
    "        if loglik[1]<-6.0:\n",
    "            print('\\nmax is:',i)\n",
    "        '''\n",
    "        if loglik_max < threshold:\n",
    "            count += 1\n",
    "        \n",
    "    return count\n",
    "\n",
    "def estimate(exp_dir, param_filename, stdout=None, stderr=None):\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "   \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    #events = make_data('s036',12000,12400,data_fileName)\n",
    "    #events = make_data('s047',16000,16400,data_fileName)\n",
    "    #events = make_data('s052',18000,18400,data_fileName)\n",
    "    #events = make_data('s032',10400,10800,data_fileName)\n",
    "    #events = make_data('s010',2400,2800,data_fileName)\n",
    "    #events = make_data('s008',2000,2400,data_fileName)\n",
    "    #events = make_data('s007',1600,2000,data_fileName)\n",
    "    #events = make_data('s005',1200,1600,data_fileName)\n",
    "    #events = make_data('s004',800,1200,data_fileName)\n",
    "    #events = make_data('s003',400,800,data_fileName)\n",
    "    events = make_data('s002',0,400,data_fileName)\n",
    "    n_jumps_per_dim = list(map(len, events[0]))\n",
    "    n_nodes = len(events[0])\n",
    "    #print('Number of jumps:', len(events)*sum(n_jumps_per_dim))\n",
    "    #print('per node:', n_jumps_per_dim)\n",
    "    \n",
    "    events = torch.tensor(events, dtype=torch.float32)\n",
    "    \n",
    "    #print('\\nestimating')\n",
    "    #print('=========')\n",
    "\n",
    "    param_filename = os.path.join(exp_dir, param_filename)\n",
    "    if not os.path.exists(param_filename):\n",
    "        raise FileNotFoundError(\n",
    "            'Input file `{:s}` not found.'.format(param_filename))\n",
    "    with open(param_filename, 'r') as param_file:\n",
    "        param_dict = json.load(param_file)\n",
    "        \n",
    "    mu = torch.tensor(param_dict['vi_exp']['mu'],dtype=torch.float32)\n",
    "    W = torch.tensor(param_dict['vi_exp']['adjacency'],dtype=torch.float32)\n",
    "    \n",
    "    #W = W.view(n_nodes,n_nodes)\n",
    "    \n",
    "    #print('mu:',mu)\n",
    "    #print('W:',W)\n",
    "    \n",
    "    nu =  -1.2595559021522644\n",
    "    sigma = 0.0626264422278299\n",
    "\n",
    "    \n",
    "    #global result\n",
    "    threshold = -8.7\n",
    "    for i in range(1000):\n",
    "        count = decision_fun(mu=mu, W=W, new_events=events, M=1 ,param_dict=param_dict_exitation,nu=nu, sigma=sigma, threshold=threshold)\n",
    "        if count <= 20:\n",
    "            break\n",
    "        threshold -=0.02\n",
    "        print('i is:',i)\n",
    "        print('count is:',count)\n",
    "    #print(result)\n",
    "    # Log that the run is finished\n",
    "    print('count is',count)\n",
    "\n",
    "    print('false alarm is:',count/400)\n",
    "    print('threshold is:', threshold)\n",
    "    \n",
    "    all_events = make_data_all(data_fileName)\n",
    "    all_events = torch.tensor(all_events, dtype=torch.float32)\n",
    "    counts = decision_fun(mu=mu, W=W, new_events=all_events, M=1 ,param_dict=param_dict_exitation,nu=nu, sigma=sigma, threshold=threshold)\n",
    "    print('counts is',counts)\n",
    "    print('miss rate:',(20400-counts-(400-count))/((400-count)+(20400-counts-(400-count))) )\n",
    "    \n",
    "    print('\\n\\nFinished.')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dir', dest='dir', type=str,\n",
    "                        #required=True, help=\"Working directory\")\n",
    "                        required=False, default=\".\")\n",
    "    parser.add_argument('-p', '--params', dest='param_filename', type=str,\n",
    "                        required=False, default='params.json',\n",
    "                        help=\"Input parameter file (JSON)\")\n",
    "    args = parser.parse_known_args()[0]\n",
    "    estimate ('.','./result/decay0.3+1e-04+c1+epsilon=-3.0/s002_0-200.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
