{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "\n",
      "Experiment parameters\n",
      "=====================\n",
      "        exp_dir = .\n",
      " param_filename = params.json\n",
      "output_filename = penalty10_decay1.0+s052_18000-18020.json\n",
      "\n",
      "\n",
      "Start time is:  2020-03-18 10:02:07.125415\n",
      "\n",
      "Number of jumps: 440\n",
      "\n",
      "per node: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "INFERENCE\n",
      "=========\n",
      "\n",
      "Run VI (vi_exp)\n",
      "------\n",
      "vi random seed: 29694\n",
      "end_iter is: 37447\n",
      "Converged!\n",
      "\n",
      "\n",
      "\n",
      "Save results...\n",
      "\n",
      "coeffs: [-0.6139538761429604, -3.6464786754928435, -4.199939052776514, -4.387375802890577, -3.705215327902025, -4.236425080101502, -4.402173194590749, -4.471575526180019, -4.443119180814224, -4.4538779633298695, -4.332907806241744, -3.2574440797988484, -4.394638847561238, -4.398173999122096, -4.397163454006465, -4.314792144685461, -4.249986793864624, -4.186790827836336, -4.144061905351146, -3.9984484910947113, -3.941975225812695, -3.030536889244926, -0.23473779110994297, -3.6900432885332255, -3.7096845031270966, -4.371674917275933, -4.324583259449564, -4.243665590479949, -4.173046636395878, -4.139117208723083, -4.001165231392536, -3.962365346815113, -3.0176314053962683, -3.2884938800864565, -0.21949813511215652, -4.082158602129802, -4.298869683437255, -4.329066142022067, -4.263187094782982, -4.205066570688748, -4.141381550516714, -3.9901602930515403, -3.9441396220603355, -3.011705277749192, -3.8099960778846818, -0.7759516055181347, -1.0425799806428213, -4.040392144470881, -4.332861755344865, -4.242457470280104, -4.187352050644352, -4.14384243924062, -4.005027065014925, -3.9575152299214444, -3.022631373222018, -3.950090052829789, -2.8414688959317744, -2.4592353999034144, -0.42006641352001256, -3.5565413326713684, -4.254191157571419, -4.176273548823763, -4.162356061734761, -3.9882977342645862, -3.969331646153655, -3.032007229130399, -4.200160696292416, -3.9634994356664768, -3.889629362806711, -3.7890386204262163, -0.1702131591551429, -3.755001055527223, -4.188471749627584, -4.1346859171213035, -3.9790791239307426, -3.9417798250035965, -3.0278194006031294, -4.313949971042729, -4.141891855226842, -4.119484385674813, -4.088989183916238, -3.4278517352868945, -0.10453947113099661, -3.793888943966262, -4.160532745943009, -4.005626477224725, -3.9537954872435437, -3.0285698602669826, -4.351356408909533, -4.229848365192815, -4.2020619466502, -4.16671317403988, -3.80349309872908, -3.1161041743035267, -0.06377206452077419, -3.8063837539369687, -4.014784840043485, -3.9461006692517864, -3.0352309816249607, -4.350624567983492, -4.27270294260492, -4.229694625438062, -4.203510689740699, -3.916549018800676, -3.455552720710974, -2.69318557454886, -0.0828199240916159, -3.5150555093905234, -3.4606947739684633, -3.0251982331167806, -4.3427188426375904, -4.259054302263327, -4.268853252143993, -4.232346278965166, -3.9649833982149896, -3.567163911017531, -2.922343126621277, -2.1258173083849585, -0.04122460626264586, -3.642392925297757, -3.041867794393054, -4.383210468939212, -4.321996186997249, -4.274967936503533, -4.28760596385037, -4.129131666637962, -3.8985453130312795, -3.606331078830516, -3.345451665059325, -2.0824906190696684, -0.16689632039169988, -0.5279424306044979, -1.928515602372292, -0.09588091737584782, -0.020501961494306296, -0.02291273874519113, -0.07952257300463235, -0.003213999574229415, -0.00918450828322787, 0.010556646732666833, 0.0010607824038506566, -0.015178824708168, -0.017785310259043577, -0.09279592296756621, 0.010743738323791377, -0.011813242668691033, 0.006865528399463322, -0.0035499284674040967, 0.009086812587382342, -0.0052094472047150816, 0.009086673994228744, 0.0018115259438446598, 0.0168389138177479, 0.014448293939901751, -1.8402107366889804, -0.05194596353308524, -0.017324812756992998, 0.010744430391736517, 0.0008391296501254878, 0.02226141245558327, -0.003860402990763775, 0.012066284125649682, 0.006121933547415541, 0.011701205709638207, 0.019136821956542708, -0.0900973992801667, -1.8473871018624226, -0.01039989731758314, -0.010409176426719693, 0.0031414530566815616, 0.006371235728581845, 0.010674494057308153, 0.0016486258375704621, 0.00033137570577630694, 0.002334616754583227, -0.000943702037633314, -0.017577702841382215, -1.3108517621953644, -1.1025346441380068, -0.00021750707356278933, 0.006246728859378119, 0.020092559844532402, -0.005899089421182545, -0.013542890504705573, 0.008144535126857854, -0.005854691879399371, 0.0007296757361489327, -0.013629476541966482, -0.1734168223540963, -0.26550612782254485, -1.6353258076728745, -0.06461338580849785, 0.010523286279906455, 0.012531407323542925, 0.0016524547461494892, -0.011999202304219723, -0.01711664329305095, -0.0026477219579529475, 0.02195955289472928, -0.021566571017697753, -0.008167316828242866, -0.0009792439737097865, -1.7976656204284465, -0.016146636420050445, -0.009544440499447599, 0.02398609665067972, 0.0066524668051389955, 0.010808260490314574, -0.0071770457780503056, 0.01091091690706567, 0.01406245467453879, 0.0037305042844352695, 0.003687012617082563, -0.021809558024734537, -1.8212371731378172, -0.023395998282104265, -0.0013089164151776516, -0.007294124518874867, 0.0034884648833809115, 0.004019426598329765, -0.015920624209851506, -0.0026729037541969518, -0.0019286003724160031, -0.016178454181785082, -0.024913078760686953, -0.08339601211675376, -1.7729617346844218, -0.007717458977744682, -0.007701485000110878, 0.004115798545081601, -0.01221786623984967, 0.00653686806269291, 0.007524099679494057, 0.018197362514436863, 0.008250318276230123, 0.012486482079142856, -0.02891495704868362, -0.14098737533109176, -1.7173880154049703, -0.014876133002917305, -0.017906097241850678, 0.012963701448704167, 0.02057903190802437, 0.016970560923026946, -0.0017826412408062148, 0.013419956850837788, -0.00572872116801078, -0.028333840050561266, -0.10581090624352794, -0.32746651100546015, -1.6190251980004435, -0.004058012509151634, -0.006321978333940532, 0.011175956560732822, 0.015982480285931383, 0.028528736349131497, -0.009337844776857408, 0.004319956455505245, -0.017216712902960302, -0.018536838380880966, -0.014452980184766416, -0.2872301490357054, -1.4989667020605684, -0.6946303027106008]\n",
      "\n",
      "adjacency: [0.016771795821986683, 0.004443295969503534, 0.004631731836701145, 0.004467218986110562, 0.004953219884319218, 0.005152225280522208, 0.00564815677198887, 0.0057279281471609945, 0.006723959314996797, 0.006900050340655319, 0.017251519813690195, 0.77109003341992, 0.01013899914560773, 0.009319667730417076, 0.004546505666872006, 0.004862206519163773, 0.005045823381698706, 0.005711031867594676, 0.0057213909848106955, 0.006647699915684191, 0.006832642778856083, 0.017306953537536838, 0.016186116701461577, 0.7832125218196563, 0.006335581795064539, 0.005101265991172545, 0.004818132727119944, 0.0051127845704292355, 0.005371516119772474, 0.005830315029728371, 0.006800063938448033, 0.00709177389180694, 0.018136665822431487, 0.008434275614339633, 0.42800065523874964, 0.31574420623276334, 0.0064740252392579715, 0.004769882620080933, 0.005074836674815996, 0.005652693072374525, 0.005993257519168384, 0.006594960214833974, 0.007112694496317898, 0.01787965526209923, 0.0072758162559261996, 0.028770871359882287, 0.047489585133157736, 0.6325170621006299, 0.011851320364203496, 0.005115599044219522, 0.005507452652828056, 0.005709256702611423, 0.006980847148574629, 0.007185589700933912, 0.017832622375070627, 0.00527351307128429, 0.0072897699094362, 0.007647110492963666, 0.008336748147108915, 0.8206449300948835, 0.008886485472252724, 0.005687048357411695, 0.005606506958703467, 0.006788855135423404, 0.00698750534485886, 0.0180688134997697, 0.004815036552110105, 0.005682204508197574, 0.005934495273461837, 0.0061187927546638545, 0.012460766500928788, 0.877457341933947, 0.00866750056122746, 0.005753664420416907, 0.006797882440296577, 0.007007342069816535, 0.01765667973777543, 0.00489267174713307, 0.0053829602645509515, 0.00552643589222276, 0.005887792439041824, 0.008609509750719361, 0.019015900279763447, 0.9115452523729004, 0.008303589865056011, 0.006741317562844452, 0.007052548493143789, 0.018113615523073133, 0.004683150840597081, 0.0050525362391239705, 0.005160304205314458, 0.005406527745737296, 0.007141471585592547, 0.01228516080749906, 0.031825790234274245, 0.8913193015017565, 0.011268528976272403, 0.011968031218773758, 0.017396875203139688, 0.004586052529391379, 0.005023753093867008, 0.005167871808142338, 0.005197645068658096, 0.007058015607490591, 0.010975695618275067, 0.023954146887894885, 0.07098446964072325, 0.9226872617263014, 0.009712809027505499, 0.017786715212466032, 0.004490397470196142, 0.004726938611122669, 0.00482625443017317, 0.005148223452933663, 0.005870537915173617, 0.00771415773582421, 0.010358620408114303, 0.013340383570906696, 0.07096991225978161, 0.8051019328681135, 0.4596904419718108]\n",
      "\n",
      "mu: [0.5298907026174964, 0.011424648291283366, 0.005743052489921899, 0.004783491996730445, 0.010481188727146535, 0.005353430565672719, 0.004589556603497717, 0.0041158360994245115, 0.004316790909769447, 0.004409577144777529, 0.005001775220752299]\n",
      "\n",
      "\n",
      "Finished.\n",
      "\n",
      "End time is:  2020-03-18 12:19:17.948716\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.optimize import fsolve\n",
    "from scipy import log\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "# External libraries\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Internal libraries\n",
    "\n",
    "import models\n",
    "import posteriors\n",
    "import priors\n",
    "import hawkes_model, excitation_kernels\n",
    "import learners\n",
    "import utils\n",
    "\n",
    "from make_data_for_samples import make_data              #多个样本数据\n",
    "from make_data_for_estimate import make_estimate_data    #单个样本数据\n",
    "\n",
    "def make_object(module, name, args):\n",
    "    return getattr(module, name)(**args)\n",
    "\n",
    "#def learn_vi(events, end_time, vi_seed, adjacency_true, inference_param_dict, return_learner=False):\n",
    "def learn_vi(events, vi_seed, inference_param_dict, return_learner=False):\n",
    "    # Extract some parameters for easier access\n",
    "    #n_nodes = len(events)\n",
    "    n_events = len(events)\n",
    "    n_nodes = len(events[0])\n",
    "    M = inference_param_dict['excitation']['args'].get('M', 1)\n",
    "    n_params = n_nodes * (n_nodes * M + 1)\n",
    "    n_edges = M * n_nodes ** 2\n",
    "    # Set seed\n",
    "    np.random.seed(vi_seed)\n",
    "    # Set starting pointM * n_nodes ** 2\n",
    "    x0 = torch.tensor(\n",
    "        np.hstack((\n",
    "            np.hstack((  # alpha, the mean of the parameters\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_nodes),\n",
    "                np.random.normal(loc=0.1, scale=0.1, size=n_edges),)),\n",
    "            np.hstack((  # beta=log(sigma), log of the variance of the parameters\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_nodes), 1e-1, 2.0)),\n",
    "                np.log(np.clip(np.random.normal(loc=0.2, scale=0.1, size=n_edges), 1e-1, 2.0)),))\n",
    "        )),\n",
    "        dtype=torch.float64, requires_grad=True\n",
    "    )\n",
    "    # Init Hawkes process model object\n",
    "    excitation_obj = make_object(excitation_kernels, **inference_param_dict['excitation'])\n",
    "    hawkes_model_obj = hawkes_model.HawkesModel(excitation=excitation_obj, verbose=False)\n",
    "    # Init the posterior object\n",
    "    posterior_obj = make_object(posteriors, **inference_param_dict['posterior'])\n",
    "    # Init the prior object\n",
    "    prior_type = inference_param_dict['prior']['name']\n",
    "    prior_args = inference_param_dict['prior']['args']\n",
    "    prior_args['C'] = torch.tensor(prior_args['C'], dtype=torch.float64)  # cast to tensor\n",
    "    prior_obj = make_object(priors, prior_type, prior_args)\n",
    "    # Init the variational inference model object\n",
    "    model = models.ModelHawkesVariational(\n",
    "        model=hawkes_model_obj, posterior=posterior_obj, prior=prior_obj,\n",
    "        **inference_param_dict['model']['args'])\n",
    "   \n",
    "    # Init the optimizer\n",
    "    opt_type = inference_param_dict['optimizer']['name']\n",
    "    opt_args = inference_param_dict['optimizer']['args']\n",
    "    opt = getattr(optim, opt_type)([x0], **opt_args)\n",
    "    # Init learner\n",
    "    learner = learners.VariationalInferenceLearner(\n",
    "        model=model, optimizer=opt, **inference_param_dict['learner']['args'])\n",
    "    # Fit the model\n",
    "    events_t = [torch.tensor(events_i) for events_i in events]  # cast to tensor\n",
    "    #learner.fit(events_t, end_time, x0, callback=callback)\n",
    "    learner.fit(events_t, x0=x0, callback=None)\n",
    "    print()\n",
    "    if return_learner:\n",
    "        return learner\n",
    "    # Extract the mode of the posterior\n",
    "    z_est_mode = learner.model.posterior.mode(learner.coeffs[:n_params], learner.coeffs[n_params:])\n",
    "    adj_est_ora = z_est_mode[n_nodes:].detach()\n",
    "    mu_est_ora = z_est_mode[:n_nodes].detach()\n",
    "    adj_est_ora = adj_est_ora.view(n_nodes, n_nodes, M)\n",
    "    adj_est = z_est_mode[n_nodes:].detach().numpy()\n",
    "    adj_est = np.reshape(adj_est, (n_nodes, n_nodes, M)).sum(-1).ravel()\n",
    "    mu_est = z_est_mode[:n_nodes].detach().numpy()\n",
    "    #mu_est = np.reshape(mu_est,n_nodes).ravel()\n",
    "    coeffs_est = learner.coeffs.detach().numpy()\n",
    "    #log_like_sum,min_intens,log_like,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    log_like_sum,intens_sum,integral_instesity,end_time = hawkes_model_obj.log_likelihood(mu_est_ora,adj_est_ora)\n",
    "    \n",
    "    return coeffs_est, adj_est,mu_est,intens_sum,integral_instesity,end_time\n",
    "\n",
    "\n",
    "def run(exp_dir, param_filename, output_filename, stdout=None, stderr=None):\n",
    "    # Reset random seed\n",
    "    np.random.seed(None)\n",
    "\n",
    "    if stdout is not None:\n",
    "        sys.stdout = open(stdout, 'w')\n",
    "    if stderr is not None:\n",
    "        sys.stderr = open(stderr, 'w')\n",
    "\n",
    "    print('\\nExperiment parameters')\n",
    "    print('=====================')\n",
    "    print(f'        exp_dir = {exp_dir:s}')\n",
    "    print(f' param_filename = {param_filename:s}')\n",
    "    print(f'output_filename = {output_filename:s}')\n",
    "    print(flush=True)\n",
    "    print('\\nStart time is: ', datetime.datetime.today())\n",
    "\n",
    "    result_dict = {}\n",
    "    \n",
    "    data_fileName = \"./data/DSL-StrongPasswordData.xls\"\n",
    "    global events\n",
    "    events = make_data('s052',18000,18020,data_fileName)\n",
    "    n_jumps_per_dim = list(map(len, events[0]))\n",
    "    print('\\nNumber of jumps:', len(events)*sum(n_jumps_per_dim))\n",
    "    print('\\nper node:', n_jumps_per_dim)\n",
    "    \n",
    "    C_list = [1.0]*132\n",
    "    \n",
    "    param_dict={'inference':{'vi_exp':{'excitation': {'name': 'ExponentialKernel','args': {'decay': 1.0, 'cut_off': 1000.0}}, \n",
    "                          'posterior': {'name': 'LogNormalPosterior', 'args': {}},\n",
    "                          'prior': {'name': 'GaussianLaplacianPrior', 'args': {'dim': 11, 'n_params': 132, 'C': C_list}}, \n",
    "                          'model': {'args': {'n_samples': 1, 'n_weights': 1, 'weight_temp': 1.0}}, \n",
    "                          'optimizer': {'name': 'Adam', 'args': {'lr': 0.01}}, \n",
    "                          'learner': {'args': {'tol': 1e-04, 'lr_gamma': 0.9999, 'max_iter': 40000, 'hyperparam_momentum': 0.5, 'hyperparam_interval': 100, 'hyperparam_offset': 0}}}}\n",
    "               }\n",
    "    \n",
    "\n",
    "    print('\\nINFERENCE')\n",
    "    print('=========')\n",
    "\n",
    "    for key, inference_param_dict in param_dict['inference'].items():\n",
    "        if key.startswith('vi'):\n",
    "            print(f'\\nRun VI ({key:s})')\n",
    "            print('------')\n",
    "            # Set random seed (for reproducibility)\n",
    "            np.random.seed()  # Reset random number generator to avoid dependency on simulation seed\n",
    "            #vi_seed = np.random.randint(2**32 - 1)\n",
    "            vi_seed = np.random.randint(2**16 - 1)\n",
    "            print(f'vi random seed: {vi_seed}')\n",
    "            # Run inference\n",
    "            global intens_sum\n",
    "            global integral_instesity\n",
    "            #coeffs_var, adj_var, mu_var, nu, varsigma = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            coeffs_var, adj_var, mu_var,intens_sum,integral_instesity,end_time  = learn_vi(events, vi_seed, inference_param_dict)\n",
    "            #模型参数\n",
    "            adj_var = adj_var.ravel()\n",
    "            mu_var = mu_var.ravel()            \n",
    "          \n",
    "            global  end_time_result\n",
    "            end_time_result = [0.0]*len(end_time)\n",
    "            \n",
    "            for i in range(len(end_time)):\n",
    "                end_time_result[i]=end_time[i].tolist()\n",
    "                \n",
    "            expresion_temp = ''\n",
    "            events_n = len(events)\n",
    "            dim = len(events[0])\n",
    "            \n",
    "            for i in range(events_n):\n",
    "                temp = ''                  \n",
    "                for j in range(dim):\n",
    "                    temp += 'log('+str(intens_sum[i][j].detach().numpy())+ '-'+ 'epsilon_noise['+str(i)+'])'+ '+'   \n",
    "                #print(intens)\n",
    "                expresion_temp += temp[:-1] + '-' + str(integral_instesity[i].detach().numpy()) + '+' + str(dim) +'*'+ str(end_time_result[i])+'*'+ 'epsilon_noise['+str(i)+'],'\n",
    "\n",
    "            expresion = expresion_temp[:-1]\n",
    "            expresion = '['+expresion+']'\n",
    "         \n",
    "            result_dict.update({\n",
    "                key: {\n",
    "                    'vi_seed': vi_seed,             # VI random seed\n",
    "                    'coeffs': coeffs_var.tolist(),  # VI parameters\n",
    "                    'adjacency': adj_var.tolist(),  # VI Estimator\n",
    "                    'mu':  mu_var.tolist(),\n",
    "                    'expresion': expresion,\n",
    "                }\n",
    "            })\n",
    " \n",
    "\n",
    "    print('\\n\\nSave results...')\n",
    "    \n",
    "    print('\\ncoeffs:',  coeffs_var.tolist())\n",
    "    print( '\\nadjacency:', adj_var.tolist())\n",
    "    print('\\nmu:', mu_var.tolist())\n",
    "    #print('\\nnu:',nu)\n",
    "    #print('\\nvarsigma:',varsigma)\n",
    "\n",
    "    with open(os.path.join(exp_dir, output_filename), 'w') as output_file:\n",
    "        json.dump(result_dict, output_file)\n",
    "\n",
    "    # Log that the run is finished\n",
    "    print('\\n\\nFinished.')\n",
    "    print('\\nEnd time is: ', datetime.datetime.today())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--dir', dest='dir', type=str,\n",
    "                        #required=True, help=\"Working directory\")\n",
    "                        required=False, default=\".\")\n",
    "    parser.add_argument('-p', '--params', dest='param_filename', type=str,\n",
    "                        required=False, default='params.json',\n",
    "                        help=\"Input parameter file (JSON)\")\n",
    "    parser.add_argument('-o', '--outfile', dest='output_filename', type=str,\n",
    "                        required=False, default='output.json',\n",
    "                        help=\"Output file (JSON)\")\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    #run(exp_dir=args.dir, param_filename=args.param_filename,output_filename=args.output_filename)\n",
    "    run ('.','params.json','penalty10_decay1.0+s052_18000-18020.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
